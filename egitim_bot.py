#!/usr/bin/env python3
"""
ğŸ“š EÄÄ°TÄ°M GÃœNDEM TAKÄ°P BOTU v4.0 - YOUTUBE AI EDÄ°TION
======================================================
LGS/YKS Ã–ÄŸrenci ve Ã–ÄŸretmenler iÃ§in GÃ¼nlÃ¼k Haber & GÃ¼ndem Botu

ğŸ†• v4.0 YENÄ°LÄ°KLER:
- ğŸ¬ YOUTUBE AI VÄ°DEOLARI: PopÃ¼ler AI kanallarÄ±ndan son videolar
  â€¢ AI Explained, Two Minute Papers, Yannic Kilcher
  â€¢ Matt Wolfe, The AI Advantage, AI Jason
  â€¢ Fireship, bycloud, Prompt Engineering
  â€¢ Ve daha fazlasÄ±...
- ğŸ“º Video Ã¶zetleri ve doÄŸrudan linkler
- ğŸ”¥ Trend AI iÃ§erikleri
- TÃ¼m v3.0 Ã¶zellikleri korundu

v3.0 Ã–ZELLÄ°KLER:
- PISA liderlerinden eÄŸitim haberleri (Makao, Singapur, Estonya, Japonya, Kore...)
- Son 48 saat filtresi - taze haberler
- Yinelenen haber filtreleme
- GÃ¼ncellenmiÅŸ sÄ±nav tarihleri (2026)
- GeniÅŸletilmiÅŸ akademik kaynaklar (ERIC, Semantic Scholar, OECD)
- UluslararasÄ± deÄŸerlendirme raporlarÄ± (PISA, TIMSS)
- Makro eÄŸitim politikasÄ± haberleri
- ArXiv rate limit bypass stratejisi
- TÃ¼rkiye ulusal izleme araÅŸtÄ±rmalarÄ±

GeliÅŸtirici: Numan Hoca iÃ§in Claude tarafÄ±ndan oluÅŸturuldu
Tarih: AralÄ±k 2025
"""

import os
import requests
import feedparser
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
from bs4 import BeautifulSoup
import json
import re
import locale
import hashlib
import time
from urllib.parse import quote_plus

# TÃ¼rkÃ§e tarih formatÄ± iÃ§in
try:
    locale.setlocale(locale.LC_TIME, 'tr_TR.UTF-8')
except:
    try:
        locale.setlocale(locale.LC_TIME, 'Turkish_Turkey.1254')
    except:
        pass

# TÃ¼rkÃ§e ay ve gÃ¼n isimleri
TURKISH_MONTHS = {
    'January': 'Ocak', 'February': 'Åubat', 'March': 'Mart',
    'April': 'Nisan', 'May': 'MayÄ±s', 'June': 'Haziran',
    'July': 'Temmuz', 'August': 'AÄŸustos', 'September': 'EylÃ¼l',
    'October': 'Ekim', 'November': 'KasÄ±m', 'December': 'AralÄ±k'
}

TURKISH_DAYS = {
    'Monday': 'Pazartesi', 'Tuesday': 'SalÄ±', 'Wednesday': 'Ã‡arÅŸamba',
    'Thursday': 'PerÅŸembe', 'Friday': 'Cuma', 'Saturday': 'Cumartesi',
    'Sunday': 'Pazar'
}

def format_turkish_date(dt: datetime, include_day: bool = True) -> str:
    """Tarihi TÃ¼rkÃ§e formatta dÃ¶ndÃ¼r"""
    day = dt.day
    month = TURKISH_MONTHS.get(dt.strftime('%B'), dt.strftime('%B'))
    year = dt.year
    
    if include_day:
        weekday = TURKISH_DAYS.get(dt.strftime('%A'), dt.strftime('%A'))
        return f"{day} {month} {year}, {weekday}"
    return f"{day} {month} {year}"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API ANAHTARLARI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GEMINI_KEY = os.environ.get('GEMINI_API_KEY', '')
TELEGRAM_TOKEN = os.environ.get('TELEGRAM_TOKEN', '')
TELEGRAM_CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID', '')

# Gemini API iÃ§in
try:
    from google import genai
except ImportError:
    genai = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# YÄ°NELENEN HABER FÄ°LTRELEME
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class NewsDeduplicator:
    """Yinelenen haberleri filtrele"""
    
    def __init__(self):
        self.seen_titles: Set[str] = set()
        self.seen_hashes: Set[str] = set()
    
    def _normalize_title(self, title: str) -> str:
        """BaÅŸlÄ±ÄŸÄ± normalize et"""
        # KÃ¼Ã§Ã¼k harf, gereksiz karakterleri kaldÄ±r
        normalized = title.lower()
        normalized = re.sub(r'[^\w\s]', '', normalized)
        normalized = ' '.join(normalized.split())
        return normalized
    
    def _get_hash(self, title: str) -> str:
        """BaÅŸlÄ±k hash'i oluÅŸtur"""
        normalized = self._normalize_title(title)
        return hashlib.md5(normalized.encode()).hexdigest()[:10]
    
    def is_duplicate(self, title: str, threshold: float = 0.7) -> bool:
        """BaÅŸlÄ±k tekrar mÄ± kontrol et"""
        if not title:
            return True
        
        title_hash = self._get_hash(title)
        
        # Tam eÅŸleÅŸme
        if title_hash in self.seen_hashes:
            return True
        
        # Benzerlik kontrolÃ¼ (basit kelime Ã¶rtÃ¼ÅŸmesi)
        normalized = self._normalize_title(title)
        words = set(normalized.split())
        
        for seen in self.seen_titles:
            seen_words = set(seen.split())
            if len(words) > 0 and len(seen_words) > 0:
                overlap = len(words & seen_words) / max(len(words), len(seen_words))
                if overlap > threshold:
                    return True
        
        # Yeni baÅŸlÄ±k - kaydet
        self.seen_hashes.add(title_hash)
        self.seen_titles.add(normalized)
        return False
    
    def reset(self):
        """Filtreyi sÄ±fÄ±rla"""
        self.seen_titles.clear()
        self.seen_hashes.clear()

# Global deduplicator
deduplicator = NewsDeduplicator()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TARÄ°H FÄ°LTRELEME - SON 48 SAAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_date(date_str: str) -> Optional[datetime]:
    """FarklÄ± tarih formatlarÄ±nÄ± parse et"""
    if not date_str:
        return None
    
    formats = [
        '%a, %d %b %Y %H:%M:%S %z',
        '%a, %d %b %Y %H:%M:%S %Z',
        '%Y-%m-%dT%H:%M:%S%z',
        '%Y-%m-%dT%H:%M:%SZ',
        '%Y-%m-%d %H:%M:%S',
        '%d %b %Y %H:%M:%S',
        '%Y-%m-%d',
    ]
    
    for fmt in formats:
        try:
            return datetime.strptime(date_str.strip(), fmt)
        except:
            continue
    
    return None

def is_recent(date_str: str, hours: int = 48) -> bool:
    """Haber son X saat iÃ§inde mi?"""
    if not date_str:
        return True  # Tarih yoksa kabul et
    
    parsed = parse_date(date_str)
    if not parsed:
        return True
    
    # Timezone-aware karÅŸÄ±laÅŸtÄ±rma
    now = datetime.now()
    try:
        if parsed.tzinfo:
            parsed = parsed.replace(tzinfo=None)
    except:
        pass
    
    diff = now - parsed
    return diff.total_seconds() < (hours * 3600)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINAV TAKVÄ°MÄ° VE GERÄ° SAYIM - GÃœNCELLENMÄ°Å TARÄ°HLER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_exam_countdown() -> Dict:
    """
    LGS ve YKS sÄ±nav tarihleri ve geri sayÄ±m
    2026 RESMÄ° TARÄ°HLER
    """
    today = datetime.now()
    
    # 2026 SINAV TARÄ°HLERÄ° - GÃœNCEL
    exams = {
        'LGS 2026': {
            'date': datetime(2026, 6, 14),  # 14 Haziran 2026 Pazar
            'name': 'ğŸ“š LGS (Liselere GeÃ§iÅŸ SÄ±navÄ±)',
            'description': '8. sÄ±nÄ±f merkezi sÄ±navÄ±'
        },
        'TYT 2026': {
            'date': datetime(2026, 6, 20),  # 20 Haziran 2026 Cumartesi
            'name': 'ğŸ“ TYT (Temel Yeterlilik Testi)',
            'description': 'YKS 1. Oturum'
        },
        'AYT 2026': {
            'date': datetime(2026, 6, 21),  # 21 Haziran 2026 Pazar
            'name': 'ğŸ“– AYT (Alan Yeterlilik Testi)',
            'description': 'YKS 2. Oturum'
        },
        'YDT 2026': {
            'date': datetime(2026, 6, 21),  # 21 Haziran 2026 Pazar (AYT ile aynÄ± gÃ¼n)
            'name': 'ğŸŒ YDT (YabancÄ± Dil Testi)',
            'description': 'YKS 3. Oturum'
        },
        # YarÄ±yÄ±l tatili 2025-2026
        'YarÄ±yÄ±l Tatili': {
            'date': datetime(2026, 1, 19),
            'name': 'ğŸ–ï¸ YarÄ±yÄ±l Tatili BaÅŸlangÄ±cÄ±',
            'description': '2 hafta tatil'
        },
        # 2. DÃ¶nem
        '2. DÃ¶nem BaÅŸlangÄ±cÄ±': {
            'date': datetime(2026, 2, 2),
            'name': 'ğŸ« 2. DÃ¶nem BaÅŸlangÄ±cÄ±',
            'description': 'Okula dÃ¶nÃ¼ÅŸ'
        },
        # Yaz tatili
        'Yaz Tatili': {
            'date': datetime(2026, 6, 19),
            'name': 'â˜€ï¸ Yaz Tatili BaÅŸlangÄ±cÄ±',
            'description': 'OkullarÄ±n kapanÄ±ÅŸÄ±'
        }
    }
    
    countdown_list = []
    
    for exam_key, exam_info in exams.items():
        exam_date = exam_info['date']
        days_left = (exam_date.date() - today.date()).days
        
        if days_left >= 0:
            weeks = days_left // 7
            remaining_days = days_left % 7
            
            if days_left == 0:
                time_str = "ğŸ”´ BUGÃœN!"
            elif days_left == 1:
                time_str = "ğŸŸ¡ YARIN!"
            elif days_left <= 7:
                time_str = f"ğŸŸ  {days_left} gÃ¼n"
            elif days_left <= 30:
                time_str = f"ğŸŸ¡ {weeks} hafta {remaining_days} gÃ¼n"
            else:
                months = days_left // 30
                time_str = f"ğŸ“… {months} ay {days_left % 30} gÃ¼n ({days_left} gÃ¼n)"
            
            countdown_list.append({
                'name': exam_info['name'],
                'description': exam_info['description'],
                'date': format_turkish_date(exam_date, include_day=False),
                'days_left': days_left,
                'time_str': time_str,
                'is_exam': 'SÄ±nav' in exam_info['name'] or 'Test' in exam_info['name']
            })
    
    countdown_list = sorted(countdown_list, key=lambda x: x['days_left'])
    
    return {
        'today': format_turkish_date(today, include_day=True),
        'countdowns': countdown_list
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MEB HABERLERÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_meb_news() -> List[Dict]:
    """MEB'den son haberler"""
    news = []
    
    try:
        url = "https://www.meb.gov.tr"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        r = requests.get(url, headers=headers, timeout=15)
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            
            news_items = soup.find_all('a', class_='news-item') or \
                        soup.find_all('div', class_='haber') or \
                        soup.find_all('article')
            
            for item in news_items[:10]:
                title = item.get_text(strip=True)[:150]
                link = item.get('href', '')
                if link and not link.startswith('http'):
                    link = url + link
                
                if title and len(title) > 20 and not deduplicator.is_duplicate(title):
                    news.append({
                        'title': title,
                        'source': 'MEB',
                        'link': link,
                        'is_important': any(kw in title.lower() for kw in [
                            'lgs', 'yks', 'sÄ±nav', 'mÃ¼fredat', 'Ã¶ÄŸretmen', 
                            'tatil', 'bakan', 'atama', 'maaÅŸ'
                        ])
                    })
    except Exception as e:
        print(f"MEB haber hatasÄ±: {e}")
    
    return news

def get_education_news_turkey() -> List[Dict]:
    """TÃ¼rkiye eÄŸitim haberleri - yinelenmesiz, gÃ¼ncel"""
    news = []
    
    sources = [
        ('https://www.hurriyet.com.tr/rss/egitim', 'HÃ¼rriyet'),
        ('https://www.milliyet.com.tr/rss/rssNew/egitimRss.xml', 'Milliyet'),
        ('https://www.sabah.com.tr/rss/egitim.xml', 'Sabah'),
        ('https://www.cumhuriyet.com.tr/rss/egitim', 'Cumhuriyet'),
        ('https://www.ntv.com.tr/egitim.rss', 'NTV'),
        ('https://www.haberturk.com/rss/egitim.xml', 'HabertÃ¼rk'),
    ]
    
    important_keywords = [
        'lgs', 'yks', 'tyt', 'ayt', 'Ã¶sym', 'meb', 'sÄ±nav', 'mÃ¼fredat',
        'Ã¶ÄŸretmen', 'atama', 'maaÅŸ', 'tatil', 'okul', 'ders', 'not',
        'bakan', 'eÄŸitim', 'Ã¶ÄŸrenci', 'Ã¼niversite', 'lise', 'ortaokul',
        'beceri temelli', 'maarif modeli', 'pisa', 'timss'
    ]
    
    for rss_url, source in sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:8]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:200] if entry.get('summary') else ''
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                # Tarih kontrolÃ¼ - son 48 saat
                if not is_recent(published, hours=48):
                    continue
                
                # Yineleme kontrolÃ¼
                if deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_important = any(kw in text for kw in important_keywords)
                is_exam_related = any(kw in text for kw in ['lgs', 'yks', 'tyt', 'ayt', 'Ã¶sym', 'sÄ±nav'])
                
                news.append({
                    'title': title[:120],
                    'summary': summary,
                    'source': source,
                    'link': link,
                    'published': published,
                    'is_important': is_important,
                    'is_exam_related': is_exam_related
                })
        except Exception as e:
            continue
    
    news = sorted(news, key=lambda x: (x['is_exam_related'], x['is_important']), reverse=True)
    return news[:12]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MATEMATÄ°K HABERLERÄ° - GÃœNCELLENMÄ°Å
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_math_news() -> List[Dict]:
    """Matematik alanÄ±ndaki son geliÅŸmeler - son 48 saat"""
    news = []
    
    world_sources = [
        ('https://www.quantamagazine.org/mathematics/feed/', 'Quanta Magazine'),
        ('https://www.sciencedaily.com/rss/computers_math/mathematics.xml', 'Science Daily'),
        ('https://phys.org/rss-feed/mathematics-news/', 'Phys.org'),
        ('https://www.ams.org/rss/mathfeed.xml', 'AMS'),
        ('https://www.maa.org/rss.xml', 'MAA'),
        ('https://plus.maths.org/content/rss.xml', 'Plus Magazine'),
    ]
    
    for rss_url, source in world_sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:300] if entry.get('summary') else ''
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                if not is_recent(published, hours=72):  # Matematik iÃ§in 72 saat
                    continue
                
                if deduplicator.is_duplicate(title):
                    continue
                
                news.append({
                    'title': title[:150],
                    'summary': summary,
                    'source': source,
                    'link': link,
                    'region': 'DÃ¼nya',
                    'needs_translation': True
                })
        except Exception as e:
            continue
    
    return news[:8]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# YAPAY ZEKA VE EÄÄ°TÄ°M HABERLERÄ° - GENÄ°ÅLETÄ°LMÄ°Å
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_ai_education_news() -> List[Dict]:
    """
    Yapay zeka, LLM geliÅŸmeleri ve eÄŸitim teknolojisi haberleri
    Ã‡oklu kaynak - tek kaynaÄŸa baÄŸÄ±mlÄ± deÄŸil
    """
    news = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. BÃœYÃœK DÄ°L MODELLERÄ° (LLM) VE AI GELÄ°ÅMELERÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    llm_sources = [
        # Ana AI ÅŸirket bloglarÄ±
        ('https://openai.com/blog/rss/', 'OpenAI', 'LLM'),
        ('https://www.anthropic.com/rss.xml', 'Anthropic', 'LLM'),
        ('https://blog.google/technology/ai/rss/', 'Google AI', 'LLM'),
        ('https://ai.meta.com/blog/rss/', 'Meta AI', 'LLM'),
        ('https://blogs.microsoft.com/ai/feed/', 'Microsoft AI', 'LLM'),
        
        # AI Haber siteleri
        ('https://www.artificialintelligence-news.com/feed/', 'AI News', 'AI Haber'),
        ('https://venturebeat.com/category/ai/feed/', 'VentureBeat AI', 'AI Haber'),
        ('https://www.technologyreview.com/feed/', 'MIT Tech Review', 'AI Haber'),
        ('https://techcrunch.com/category/artificial-intelligence/feed/', 'TechCrunch AI', 'AI Haber'),
        ('https://www.wired.com/feed/tag/ai/latest/rss', 'WIRED AI', 'AI Haber'),
        ('https://www.theverge.com/rss/ai-artificial-intelligence/index.xml', 'The Verge AI', 'AI Haber'),
        ('https://arstechnica.com/tag/artificial-intelligence/feed/', 'Ars Technica AI', 'AI Haber'),
        
        # AI AraÅŸtÄ±rma
        ('https://deepmind.google/blog/rss.xml', 'DeepMind', 'AraÅŸtÄ±rma'),
        ('https://bair.berkeley.edu/blog/feed.xml', 'Berkeley AI', 'AraÅŸtÄ±rma'),
        ('https://huggingface.co/blog/feed.xml', 'Hugging Face', 'AraÅŸtÄ±rma'),
    ]
    
    # LLM ve AI anahtar kelimeleri
    llm_keywords = [
        # Model isimleri
        'gpt', 'gpt-4', 'gpt-5', 'chatgpt', 'claude', 'gemini', 'llama', 'mistral',
        'copilot', 'deepseek', 'qwen', 'phi', 'o1', 'o3', 'sonnet', 'opus', 'haiku',
        # Teknik terimler
        'large language model', 'llm', 'transformer', 'neural network',
        'machine learning', 'deep learning', 'artificial intelligence',
        'generative ai', 'genai', 'foundation model', 'multimodal',
        'fine-tuning', 'prompt', 'reasoning', 'chain of thought',
        'rag', 'retrieval', 'embedding', 'context window', 'token',
        # Yetenekler
        'coding', 'code generation', 'text generation', 'image generation',
        'voice', 'speech', 'vision', 'video', 'agent', 'tool use', 'agentic',
        # Åirketler
        'openai', 'anthropic', 'google ai', 'meta ai', 'microsoft ai',
        'deepmind', 'hugging face', 'stability ai', 'midjourney', 'perplexity'
    ]
    
    for rss_url, source, category in llm_sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:300] if entry.get('summary') else ''
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                if not title:
                    continue
                
                if not is_recent(published, hours=96):  # 4 gÃ¼n
                    continue
                
                if deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_ai_related = any(kw in text for kw in llm_keywords)
                
                if is_ai_related:
                    news.append({
                        'title': title[:150],
                        'summary': summary[:200],
                        'source': source,
                        'category': category,
                        'link': link,
                        'is_llm': category == 'LLM',
                        'needs_translation': True
                    })
        except Exception as e:
            continue
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. EÄÄ°TÄ°M TEKNOLOJÄ°SÄ° (EdTech) HABERLERÄ°
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    edtech_sources = [
        ('https://www.edsurge.com/articles_rss', 'EdSurge', 'EdTech'),
        ('https://www.the74million.org/feed/', 'The 74', 'EdTech'),
        ('https://www.eschoolnews.com/feed/', 'eSchool News', 'EdTech'),
        ('https://edtechmagazine.com/k12/rss.xml', 'EdTech Magazine', 'EdTech'),
        ('https://www.techlearning.com/rss.xml', 'Tech & Learning', 'EdTech'),
        ('https://www.elearningindustry.com/feed', 'eLearning Industry', 'EdTech'),
        ('https://www.insidehighered.com/rss.xml', 'Inside Higher Ed', 'YÃ¼ksekÃ¶ÄŸretim'),
    ]
    
    edtech_keywords = [
        'ai tutor', 'ai teacher', 'ai classroom', 'ai education', 'ai learning',
        'chatgpt education', 'chatgpt school', 'chatgpt student', 'chatgpt teacher',
        'adaptive learning', 'personalized learning', 'intelligent tutoring',
        'learning analytics', 'educational technology', 'edtech',
        'online learning', 'digital learning', 'khanmigo', 'duolingo',
        'assessment', 'grading', 'feedback', 'cheating', 'plagiarism',
        'ai policy', 'ai ban', 'ai literacy'
    ]
    
    for rss_url, source, category in edtech_sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:4]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:200] if entry.get('summary') else ''
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                if not title:
                    continue
                
                if not is_recent(published, hours=72):
                    continue
                
                if deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_relevant = any(kw in text for kw in edtech_keywords)
                
                if is_relevant:
                    news.append({
                        'title': title[:150],
                        'summary': summary[:200],
                        'source': source,
                        'category': category,
                        'link': link,
                        'is_llm': False,
                        'needs_translation': True
                    })
        except Exception as e:
            continue
    
    # Ã–nce LLM haberleri, sonra EdTech
    news = sorted(news, key=lambda x: (x.get('is_llm', False)), reverse=True)
    
    # Kaynak Ã§eÅŸitliliÄŸi saÄŸla - her kaynaktan max 2
    final_news = []
    source_counts = {}
    
    for item in news:
        source = item.get('source', '')
        if source not in source_counts:
            source_counts[source] = 0
        
        if source_counts[source] < 2:
            final_news.append(item)
            source_counts[source] += 1
        
        if len(final_news) >= 12:
            break
    
    return final_news

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ¬ YOUTUBE AI VÄ°DEOLARI - POPÃœLER KANALLAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_youtube_ai_videos() -> List[Dict]:
    """
    PopÃ¼ler AI YouTube kanallarÄ±ndan son videolar
    
    NOT: YouTube RSS doÄŸrudan eriÅŸilemeyebilir (network kÄ±sÄ±tlamalarÄ±)
    Bu durumda curated/statik liste kullanÄ±lÄ±r
    """
    videos = []
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # POPÃœLER AI YOUTUBE KANALLARI VERÄ°TABANI
    # Bu liste dÃ¼zenli olarak gÃ¼ncellenebilir
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    ai_youtube_channels = [
        # â”€â”€â”€ TIER 1: EN POPÃœLER AI KANALLARI (1M+ Abone) â”€â”€â”€
        {
            'channel_id': 'UCbfYPyITQ-7l4upoX8nvctg',
            'name': 'Two Minute Papers',
            'subscribers': '1.5M+',
            'category': 'AI AraÅŸtÄ±rma',
            'description': 'Akademik AI makalelerinin kÄ±sa Ã¶zetleri',
            'url': 'https://www.youtube.com/@TwoMinutePapers',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCZHmQk67mSJgfCCTn7xBfew',
            'name': 'Fireship',
            'subscribers': '3M+',
            'category': 'Tech/AI',
            'description': 'HÄ±zlÄ± tech ve AI aÃ§Ä±klamalarÄ±, "100 seconds" serisi',
            'url': 'https://www.youtube.com/@Fireship',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCYO_jab_esuFRV4b17AJtAw',
            'name': '3Blue1Brown',
            'subscribers': '6M+',
            'category': 'Matematik/AI',
            'description': 'Neural network ve matematik gÃ¶rselleÅŸtirmeleri',
            'url': 'https://www.youtube.com/@3blue1brown',
            'lang': 'EN'
        },
        
        # â”€â”€â”€ TIER 2: AI HABER VE ANALÄ°Z KANALLARI (500K-1M) â”€â”€â”€
        {
            'channel_id': 'UCLXo7UDZvByw2ixzpQCufnA',
            'name': 'Matt Wolfe',
            'subscribers': '650K+',
            'category': 'AI AraÃ§lar',
            'description': 'HaftalÄ±k AI araÃ§larÄ± ve haberleri',
            'url': 'https://www.youtube.com/@maboroshi_studio',
            'lang': 'EN'
        },
        {
            'channel_id': 'UC5sYcThBEkKrLQqo_v1m4VQ',
            'name': 'AI Explained',
            'subscribers': '400K+',
            'category': 'AI Analiz',
            'description': 'Derinlemesine AI analizleri ve karÅŸÄ±laÅŸtÄ±rmalarÄ±',
            'url': 'https://www.youtube.com/@aiaborovere',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCUyeluBRhGPCW4rPe_UvBZQ',
            'name': 'The AI Advantage',
            'subscribers': '500K+',
            'category': 'AI AraÃ§lar',
            'description': 'AI araÃ§larÄ± kullanÄ±m rehberleri',
            'url': 'https://www.youtube.com/@aiadvantage',
            'lang': 'EN'
        },
        
        # â”€â”€â”€ TIER 3: TEKNÄ°K AI KANALLARI (200K-500K) â”€â”€â”€
        {
            'channel_id': 'UCeYvMMZLnoqOzphJJ1Ozf_Q',
            'name': 'Yannic Kilcher',
            'subscribers': '280K+',
            'category': 'AI AraÅŸtÄ±rma',
            'description': 'AI paper incelemeleri ve teknik analizler',
            'url': 'https://www.youtube.com/@YannicKilcher',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCsbr_wOE4DMjcBW4',
            'name': 'bycloud',
            'subscribers': '350K+',
            'category': 'AI AraÅŸtÄ±rma',
            'description': 'AI paper aÃ§Ä±klamalarÄ±, teknik iÃ§erik',
            'url': 'https://www.youtube.com/@bycloudAI',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCbXgNpp0jedKWcQiULLbDTA',
            'name': 'AI Foundations',
            'subscribers': '200K+',
            'category': 'AI EÄŸitim',
            'description': 'AI temelleri ve Ã¶ÄŸretici iÃ§erikler',
            'url': 'https://www.youtube.com/@ai-foundations',
            'lang': 'EN'
        },
        
        # â”€â”€â”€ TIER 4: AI UYGULAMA VE PROMPT KANALLARI â”€â”€â”€
        {
            'channel_id': 'UC4L2IXqZvLxZdaXcvfje2OQ',
            'name': 'AI Jason',
            'subscribers': '250K+',
            'category': 'AI Prompt',
            'description': 'Prompt engineering ve AI ipuÃ§larÄ±',
            'url': 'https://www.youtube.com/@AIJasonZ',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCb-bmaFpSPnJMwJJJlU2kbQ',
            'name': 'All About AI',
            'subscribers': '400K+',
            'category': 'AI AraÃ§lar',
            'description': 'KapsamlÄ± AI araÃ§ incelemeleri',
            'url': 'https://www.youtube.com/@AllAboutAI',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCWv7vMbMWH4-V0ZXdmDpPBA',
            'name': 'Prompt Engineering',
            'subscribers': '180K+',
            'category': 'AI Prompt',
            'description': 'ChatGPT ve Claude prompt teknikleri',
            'url': 'https://www.youtube.com/@engineerprompt',
            'lang': 'EN'
        },
        
        # â”€â”€â”€ TIER 5: ÅÄ°RKET VE PODCAST KANALLARI â”€â”€â”€
        {
            'channel_id': 'UCXZCJLdBC09xxGZ6gcdrc6A',
            'name': 'OpenAI',
            'subscribers': '600K+',
            'category': 'Resmi',
            'description': 'ChatGPT, GPT-4, Sora resmi duyurularÄ±',
            'url': 'https://www.youtube.com/@OpenAI',
            'lang': 'EN'
        },
        {
            'channel_id': 'UC_x5XG1OV2P6uZZ5FSM9Ttw',
            'name': 'Google',
            'subscribers': '14M+',
            'category': 'Resmi',
            'description': 'Google AI, Gemini haberleri',
            'url': 'https://www.youtube.com/@Google',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCVHFbqXqoYvEWM1Ddxl0QKg',
            'name': 'Lex Fridman',
            'subscribers': '4.5M+',
            'category': 'AI Podcast',
            'description': 'AI liderleriyle uzun rÃ¶portajlar',
            'url': 'https://www.youtube.com/@lexfridman',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCxg7CAgk4sDJ9p3EE',
            'name': 'Andrej Karpathy',
            'subscribers': '600K+',
            'category': 'AI AraÅŸtÄ±rma',
            'description': 'Eski Tesla AI direktÃ¶rÃ¼, teknik dersler',
            'url': 'https://www.youtube.com/@AndrejKarpathy',
            'lang': 'EN'
        },
        {
            'channel_id': 'UCJlfH_QMvSCUvgGW4JAbSPQ',
            'name': 'Anthropic',
            'subscribers': '50K+',
            'category': 'Resmi',
            'description': 'Claude AI resmi duyurularÄ±',
            'url': 'https://www.youtube.com/@AnthropicAI',
            'lang': 'EN'
        },
        
        # â”€â”€â”€ TÃœRKÃ‡E AI KANALLARI â”€â”€â”€
        {
            'channel_id': 'UCnjbfvqJKgqSMtNNaPJHrqg',
            'name': 'Kodlama ZamanÄ±',
            'subscribers': '200K+',
            'category': 'TÃ¼rkÃ§e AI',
            'description': 'TÃ¼rkÃ§e AI ve programlama dersleri',
            'url': 'https://www.youtube.com/@KodlamaZamani',
            'lang': 'TR'
        },
        {
            'channel_id': 'UCBTYKH9Rh3l4',
            'name': 'Sadi Evren Åeker',
            'subscribers': '500K+',
            'category': 'TÃ¼rkÃ§e AI',
            'description': 'Yapay zeka ve veri bilimi TÃ¼rkÃ§e',
            'url': 'https://www.youtube.com/@sadievrenseker',
            'lang': 'TR'
        },
    ]
    
    # Ã–nce RSS feed'den Ã§ekmeyi dene
    ai_keywords = [
        'gpt', 'gpt-4', 'gpt-5', 'chatgpt', 'claude', 'gemini', 'llama', 'mistral',
        'copilot', 'deepseek', 'qwen', 'o1', 'o3', 'sonnet', 'opus', 'sora',
        'ai', 'artificial intelligence', 'machine learning', 'deep learning',
        'neural network', 'llm', 'large language model', 'transformer',
        'generative', 'diffusion', 'multimodal', 'agent', 'rag',
        'openai', 'anthropic', 'google ai', 'meta ai', 'microsoft',
        'prompt', 'fine-tuning', 'embedding', 'reasoning', 'coding',
        'yapay zeka', 'dil modeli'
    ]
    
    rss_success = False
    
    for channel in ai_youtube_channels:
        try:
            rss_url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel['channel_id']}"
            feed = feedparser.parse(rss_url)
            
            if feed.entries:
                rss_success = True
                for entry in feed.entries[:3]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    published = entry.get('published', '')
                    
                    video_id = ''
                    if 'yt:videoId' in entry:
                        video_id = entry['yt:videoId']
                    elif link and 'watch?v=' in link:
                        video_id = link.split('watch?v=')[-1].split('&')[0]
                    
                    if not is_recent(published, hours=168):
                        continue
                    
                    if deduplicator.is_duplicate(title):
                        continue
                    
                    title_lower = title.lower()
                    is_ai_related = any(kw in title_lower for kw in ai_keywords)
                    
                    ai_focused_channels = ['Two Minute Papers', 'AI Explained', 'Matt Wolfe', 
                                           'The AI Advantage', 'AI Jason', 'Yannic Kilcher',
                                           'All About AI', 'Prompt Engineering', 'OpenAI', 
                                           'bycloud', 'Anthropic', 'Andrej Karpathy']
                    
                    if channel['name'] in ai_focused_channels or is_ai_related:
                        videos.append({
                            'title': title[:120],
                            'channel': channel['name'],
                            'subscribers': channel['subscribers'],
                            'category': channel['category'],
                            'link': link,
                            'video_id': video_id,
                            'published': published,
                            'lang': channel['lang'],
                            'source': 'rss'
                        })
                        
        except Exception as e:
            continue
    
    # RSS Ã§alÄ±ÅŸmadÄ±ysa, curated kanal listesini dÃ¶ndÃ¼r
    if not rss_success or len(videos) < 3:
        print("   ğŸ“‹ RSS eriÅŸilemiyor, kanal listesi kullanÄ±lÄ±yor...")
        
        # Curated gÃ¼ncel video Ã¶nerileri (manuel gÃ¼ncelleme gerektirir)
        curated_videos = [
            {
                'title': 'ğŸ”¥ Two Minute Papers - En son AI araÅŸtÄ±rmalarÄ±',
                'channel': 'Two Minute Papers',
                'subscribers': '1.5M+',
                'category': 'AI AraÅŸtÄ±rma',
                'link': 'https://www.youtube.com/@TwoMinutePapers',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Fireship - AI in 100 Seconds serisi',
                'channel': 'Fireship',
                'subscribers': '3M+',
                'category': 'Tech/AI',
                'link': 'https://www.youtube.com/@Fireship',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Matt Wolfe - HaftalÄ±k AI araÃ§ incelemeleri',
                'channel': 'Matt Wolfe',
                'subscribers': '650K+',
                'category': 'AI AraÃ§lar',
                'link': 'https://www.youtube.com/@maboroshi_studio',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ AI Explained - GPT, Claude, Gemini karÅŸÄ±laÅŸtÄ±rmalarÄ±',
                'channel': 'AI Explained',
                'subscribers': '400K+',
                'category': 'AI Analiz',
                'link': 'https://www.youtube.com/@aiexplained-official',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ The AI Advantage - Pratik AI kullanÄ±m rehberleri',
                'channel': 'The AI Advantage',
                'subscribers': '500K+',
                'category': 'AI AraÃ§lar',
                'link': 'https://www.youtube.com/@aiadvantage',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Yannic Kilcher - DetaylÄ± AI paper incelemeleri',
                'channel': 'Yannic Kilcher',
                'subscribers': '280K+',
                'category': 'AI AraÅŸtÄ±rma',
                'link': 'https://www.youtube.com/@YannicKilcher',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ 3Blue1Brown - Neural Network gÃ¶rselleÅŸtirmeleri',
                'channel': '3Blue1Brown',
                'subscribers': '6M+',
                'category': 'Matematik/AI',
                'link': 'https://www.youtube.com/@3blue1brown',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Lex Fridman - AI liderlerle rÃ¶portajlar',
                'channel': 'Lex Fridman',
                'subscribers': '4.5M+',
                'category': 'AI Podcast',
                'link': 'https://www.youtube.com/@lexfridman',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Andrej Karpathy - Neural network dersleri',
                'channel': 'Andrej Karpathy',
                'subscribers': '600K+',
                'category': 'AI AraÅŸtÄ±rma',
                'link': 'https://www.youtube.com/@AndrejKarpathy',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ OpenAI - Resmi duyurular (GPT, Sora)',
                'channel': 'OpenAI',
                'subscribers': '600K+',
                'category': 'Resmi',
                'link': 'https://www.youtube.com/@OpenAI',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ Anthropic - Claude AI resmi kanal',
                'channel': 'Anthropic',
                'subscribers': '50K+',
                'category': 'Resmi',
                'link': 'https://www.youtube.com/@AnthropicAI',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ All About AI - KapsamlÄ± AI araÃ§ demolarÄ±',
                'channel': 'All About AI',
                'subscribers': '400K+',
                'category': 'AI AraÃ§lar',
                'link': 'https://www.youtube.com/@AllAboutAI',
                'lang': 'EN',
                'source': 'curated'
            },
            {
                'title': 'ğŸ”¥ AI Jason - Prompt engineering teknikleri',
                'channel': 'AI Jason',
                'subscribers': '250K+',
                'category': 'AI Prompt',
                'link': 'https://www.youtube.com/@AIJasonZ',
                'lang': 'EN',
                'source': 'curated'
            },
        ]
        
        videos = curated_videos
    
    # Kanal Ã§eÅŸitliliÄŸi saÄŸla
    final_videos = []
    channel_counts = {}
    
    for video in videos:
        ch = video.get('channel', '')
        if ch not in channel_counts:
            channel_counts[ch] = 0
        
        if channel_counts[ch] < 2:
            final_videos.append(video)
            channel_counts[ch] += 1
        
        if len(final_videos) >= 15:
            break
    
    return final_videos


def get_ai_channel_recommendations() -> List[Dict]:
    """
    Takip edilmesi Ã¶nerilen AI YouTube kanallarÄ±
    Kategorize edilmiÅŸ liste
    """
    return [
        # AraÅŸtÄ±rma
        {'name': 'Two Minute Papers', 'url': 'youtube.com/@TwoMinutePapers', 'focus': 'Paper Ã¶zetleri', 'subs': '1.5M'},
        {'name': 'Yannic Kilcher', 'url': 'youtube.com/@YannicKilcher', 'focus': 'DetaylÄ± paper analizi', 'subs': '280K'},
        {'name': 'Andrej Karpathy', 'url': 'youtube.com/@AndrejKarpathy', 'focus': 'Teknik dersler', 'subs': '600K'},
        
        # AraÃ§lar
        {'name': 'Matt Wolfe', 'url': 'youtube.com/@maboroshi_studio', 'focus': 'HaftalÄ±k AI araÃ§larÄ±', 'subs': '650K'},
        {'name': 'The AI Advantage', 'url': 'youtube.com/@aiadvantage', 'focus': 'Pratik rehberler', 'subs': '500K'},
        {'name': 'All About AI', 'url': 'youtube.com/@AllAboutAI', 'focus': 'AraÃ§ demolarÄ±', 'subs': '400K'},
        
        # Haber & Analiz
        {'name': 'AI Explained', 'url': 'youtube.com/@aiexplained-official', 'focus': 'Derin analizler', 'subs': '400K'},
        {'name': 'Fireship', 'url': 'youtube.com/@Fireship', 'focus': 'HÄ±zlÄ± gÃ¼ncellemeler', 'subs': '3M'},
        
        # Podcast & RÃ¶portaj
        {'name': 'Lex Fridman', 'url': 'youtube.com/@lexfridman', 'focus': 'AI lider rÃ¶portajlarÄ±', 'subs': '4.5M'},
    ]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ† PISA LÄ°DERLERÄ°NDEN EÄÄ°TÄ°M HABERLERÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_pisa_leaders_news() -> Dict[str, List[Dict]]:
    """
    PISA 2022'de en baÅŸarÄ±lÄ± Ã¼lkelerden eÄŸitim haberleri
    SADECE eÄŸitim politikasÄ± ve okul haberleri - Ã§ok sÄ±kÄ± filtreleme
    """
    
    # PISA 2022 Top Performers
    pisa_leaders = {
        'singapore': {
            'flag': 'ğŸ‡¸ğŸ‡¬',
            'name': 'Singapur',
            'rank': '#1-2 PISA',
            'sources': [
                ('https://www.straitstimes.com/singapore/parenting-education', 'Straits Times Education'),
                ('https://www.channelnewsasia.com/rss/latest_news.xml', 'CNA'),
            ],
            # SADECE bu kelimeler geÃ§erse al
            'must_have': ['school', 'education', 'student', 'teacher', 'exam', 'curriculum', 
                         'university', 'moe', 'psle', 'o level', 'a level', 'learning', 
                         'classroom', 'tuition', 'polytechnic'],
        },
        'japan': {
            'flag': 'ğŸ‡¯ğŸ‡µ',
            'name': 'Japonya',
            'rank': '#4-5 PISA',
            'sources': [
                ('https://www.japantimes.co.jp/feed/', 'Japan Times'),
                ('https://english.kyodonews.net/rss/all.xml', 'Kyodo News'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university', 
                         'mext', 'exam', 'curriculum', 'juku', 'learning', 'classroom',
                         'elementary', 'high school', 'college'],
        },
        'korea': {
            'flag': 'ğŸ‡°ğŸ‡·',
            'name': 'GÃ¼ney Kore',
            'rank': '#6 PISA',
            'sources': [
                ('https://en.yna.co.kr/RSS/news.xml', 'Yonhap'),
                ('https://www.koreaherald.com/rss/023.xml', 'Korea Herald'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'suneung', 'csat', 'hagwon', 'curriculum', 'learning',
                         'college', 'exam', 'classroom'],
        },
        'estonia': {
            'flag': 'ğŸ‡ªğŸ‡ª',
            'name': 'Estonya',
            'rank': '#3 PISA Fen',
            'sources': [
                ('https://news.err.ee/rss', 'ERR News'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'curriculum', 'learning', 'classroom', 'exam', 'digital education',
                         'e-school', 'gymnasium'],
        },
        'hong_kong': {
            'flag': 'ğŸ‡­ğŸ‡°',
            'name': 'Hong Kong',
            'rank': '#5 PISA',
            'sources': [
                ('https://www.scmp.com/rss/91/feed', 'SCMP'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'dse', 'curriculum', 'learning', 'classroom', 'exam',
                         'education bureau'],
        },
        'chinese_taipei': {
            'flag': 'ğŸ‡¹ğŸ‡¼',
            'name': 'Tayvan',
            'rank': '#8 PISA',
            'sources': [
                ('https://focustaiwan.tw/rss', 'Focus Taiwan'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'curriculum', 'learning', 'exam', 'college', 'ministry of education'],
        },
        'finland': {
            'flag': 'ğŸ‡«ğŸ‡®',
            'name': 'Finlandiya',
            'rank': '#12 PISA',
            'sources': [
                ('https://yle.fi/rss/uutiset.rss', 'YLE'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'curriculum', 'learning', 'classroom', 'pisa', 'finnish education'],
        },
        'canada': {
            'flag': 'ğŸ‡¨ğŸ‡¦',
            'name': 'Kanada',
            'rank': '#9 PISA',
            'sources': [
                ('https://www.cbc.ca/cmlink/rss-canada', 'CBC'),
            ],
            'must_have': ['school', 'education', 'student', 'teacher', 'university',
                         'curriculum', 'learning', 'classroom', 'college', 'provincial education'],
        },
    }
    
    # Kesinlikle ALMAYACAÄIMIZ konular (eÄŸitimle alakasÄ±z)
    exclude_keywords = [
        'prison', 'jail', 'crime', 'murder', 'police', 'court', 'arrested',
        'skating', 'ice rink', 'tourist', 'hotel', 'restaurant', 'food',
        'weather', 'storm', 'earthquake', 'flood', 'fire', 'accident',
        'sports', 'football', 'basketball', 'soccer', 'olympics', 'athlete',
        'entertainment', 'movie', 'celebrity', 'concert', 'festival',
        'stock', 'market', 'business', 'trade', 'export', 'import',
        'military', 'war', 'army', 'navy', 'defense', 'weapon',
        'smoking', 'cigarette', 'alcohol', 'drug', 'casino', 'gambling',
        'covid', 'virus', 'pandemic', 'vaccine', 'hospital', 'health crisis'
    ]
    
    all_news = {}
    
    for country_code, country_info in pisa_leaders.items():
        country_news = []
        
        for source_url, source_name in country_info['sources']:
            try:
                feed = feedparser.parse(source_url)
                for entry in feed.entries[:15]:  # Daha fazla entry tara, filtreleyeceÄŸiz
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    summary = entry.get('summary', '')[:300] if entry.get('summary') else ''
                    published = entry.get('published', '')
                    
                    if not title:
                        continue
                    
                    # Tarih kontrolÃ¼
                    if not is_recent(published, hours=96):
                        continue
                    
                    # Yineleme kontrolÃ¼
                    if deduplicator.is_duplicate(title):
                        continue
                    
                    text = (title + ' ' + summary).lower()
                    
                    # 1. ZORUNLU: En az bir eÄŸitim kelimesi Ä°Ã‡ERMELÄ°
                    has_education_keyword = any(kw in text for kw in country_info['must_have'])
                    
                    if not has_education_keyword:
                        continue
                    
                    # 2. YASAK: HiÃ§bir yasak kelime Ä°Ã‡ERMEMELÄ°
                    has_excluded = any(kw in text for kw in exclude_keywords)
                    
                    if has_excluded:
                        continue
                    
                    # Filtreleri geÃ§ti - ekle
                    country_news.append({
                        'title': title[:150],
                        'source': source_name,
                        'link': link,
                        'country': country_info['name'],
                        'flag': country_info['flag'],
                        'rank': country_info['rank'],
                        'needs_translation': True
                    })
                    
                    # Her Ã¼lkeden max 2 haber
                    if len(country_news) >= 2:
                        break
                
                if len(country_news) >= 2:
                    break
                    
            except Exception as e:
                print(f"   âš ï¸ {country_info['name']} RSS hatasÄ±: {e}")
                continue
        
        if country_news:
            all_news[country_code] = country_news
    
    return all_news

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸŒ DÃœNYADAN MAKRO EÄÄ°TÄ°M HABERLERÄ°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_global_macro_education_news() -> List[Dict]:
    """
    Global eÄŸitim politikasÄ± ve reform haberleri
    Mikro deÄŸil makro seviye
    """
    news = []
    
    # UluslararasÄ± kuruluÅŸlar
    global_sources = [
        ('https://www.unesco.org/en/rss.xml', 'UNESCO', 'UluslararasÄ±'),
        ('https://blogs.worldbank.org/education/rss.xml', 'World Bank Education', 'UluslararasÄ±'),
        ('https://www.oecd-ilibrary.org/rss/content/subject/education.xml', 'OECD', 'UluslararasÄ±'),
        ('https://www.weforum.org/agenda/feed', 'World Economic Forum', 'Global'),
        ('https://www.brookings.edu/topic/education/feed/', 'Brookings', 'Policy'),
        ('https://www.theguardian.com/education/rss', 'Guardian Education', 'UK'),
        ('https://www.nytimes.com/svc/collections/v1/publish/www.nytimes.com/section/education/rss.xml', 'NYT Education', 'US'),
    ]
    
    macro_keywords = [
        'education policy', 'education reform', 'curriculum reform',
        'national assessment', 'pisa', 'timss', 'international comparison',
        'education budget', 'teacher shortage', 'education crisis',
        'ai in education', 'digital transformation', 'education inequality',
        'higher education', 'vocational training', 'lifelong learning',
        'education minister', 'education law', 'education system'
    ]
    
    for rss_url, source, category in global_sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:6]:
                title = entry.get('title', '')
                link = entry.get('link', '')
                published = entry.get('published', '')
                summary = entry.get('summary', '')[:200] if entry.get('summary') else ''
                
                if not is_recent(published, hours=72):
                    continue
                
                if deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_macro = any(kw in text for kw in macro_keywords)
                
                if is_macro:
                    news.append({
                        'title': title[:150],
                        'source': source,
                        'category': category,
                        'link': link,
                        'needs_translation': True
                    })
        except Exception as e:
            continue
    
    return news[:8]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“š BÄ°LÄ°MSEL MAKALELER - GENÄ°ÅLETÄ°LMÄ°Å KAYNAKLAR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_arxiv_papers_safe() -> List[Dict]:
    """
    arXiv'den makaleler - RSS ile (API key gerektirmez)
    Rate limit iÃ§in bekleme sÃ¼reli
    """
    papers = []
    
    # arXiv RSS kategorileri - eÄŸitim odaklÄ±
    arxiv_categories = [
        ('http://export.arxiv.org/rss/cs.CY', 'cs.CY', 'Bilgisayar & Toplum'),  # Education papers here
        ('http://export.arxiv.org/rss/cs.AI', 'cs.AI', 'Yapay Zeka'),
        ('http://export.arxiv.org/rss/cs.CL', 'cs.CL', 'DoÄŸal Dil Ä°ÅŸleme'),
        ('http://export.arxiv.org/rss/cs.LG', 'cs.LG', 'Makine Ã–ÄŸrenmesi'),
    ]
    
    # EÄŸitim ile ilgili anahtar kelimeler
    education_keywords = [
        'education', 'learning', 'student', 'teacher', 'classroom',
        'tutoring', 'assessment', 'curriculum', 'pedagogy', 'school',
        'adaptive learning', 'intelligent tutoring', 'educational',
        'e-learning', 'mooc', 'personalized learning', 'teaching'
    ]
    
    for rss_url, category, category_name in arxiv_categories:
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:8]:
                title = entry.get('title', '').replace('\n', ' ')
                summary = entry.get('summary', '')[:500] if entry.get('summary') else ''
                link = entry.get('link', '')
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                # EÄŸitim ile ilgili mi kontrol et
                text = (title + ' ' + summary).lower()
                is_education_related = any(kw in text for kw in education_keywords)
                
                papers.append({
                    'title': title[:200],
                    'summary': summary[:300],
                    'link': link,
                    'category': category_name,
                    'arxiv_cat': category,
                    'is_education_related': is_education_related,
                    'source': 'arXiv',
                    'needs_translation': True
                })
            
            time.sleep(2)  # Rate limit iÃ§in bekleme
            
        except Exception as e:
            print(f"arXiv RSS hatasÄ± ({category}): {e}")
            continue
    
    # EÄŸitim ile ilgili olanlarÄ± Ã¶ne al
    papers = sorted(papers, key=lambda x: x.get('is_education_related', False), reverse=True)
    
    return papers[:8]

def get_eric_papers() -> List[Dict]:
    """
    ERIC benzeri kaynaklar - RSS ile (API key gerektirmez)
    EÄŸitim araÅŸtÄ±rma dergileri
    """
    papers = []
    
    # EÄŸitim araÅŸtÄ±rma dergileri RSS (ERIC yerine)
    sources = [
        ('https://bera-journals.onlinelibrary.wiley.com/feed/14678535/most-recent', 'British Journal of Educational Technology'),
        ('https://www.tandfonline.com/feed/rss/cjem20', 'Journal of Education for Teaching'),
        ('https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=10648&channel-name=Educational+Psychology+Review', 'Educational Psychology Review'),
        ('https://journals.sagepub.com/action/showFeed?ui=0&mi=ehikzz&ai=2b4&jc=rera&type=etoc&feed=rss', 'Review of Educational Research'),
    ]
    
    education_keywords = [
        'education', 'learning', 'student', 'teacher', 'assessment',
        'curriculum', 'pedagogy', 'instruction', 'classroom', 'school',
        'achievement', 'performance', 'technology', 'digital', 'online'
    ]
    
    for rss_url, source_name in sources:
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:5]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:300] if entry.get('summary') else ''
                link = entry.get('link', '')
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_relevant = any(kw in text for kw in education_keywords)
                
                if is_relevant:
                    papers.append({
                        'title': title[:200],
                        'summary': summary,
                        'link': link,
                        'source': source_name,
                        'category': 'EÄŸitim AraÅŸtÄ±rmasÄ±',
                        'needs_translation': True
                    })
        except Exception as e:
            print(f"EÄŸitim dergisi RSS hatasÄ± ({source_name}): {e}")
            continue
    
    return papers[:5]

def get_semantic_scholar_papers() -> List[Dict]:
    """
    AI & EÄŸitim makaleleri - RSS kaynaklarÄ± ile (API key gerektirmez)
    """
    papers = []
    
    # AI ve EÄŸitim odaklÄ± RSS kaynaklarÄ±
    sources = [
        ('https://www.jair.org/index.php/jair/gateway/plugin/WebFeedGatewayPlugin/rss2', 'Journal of AI Research'),
        ('https://ieeexplore.ieee.org/rss/TOC42.XML', 'IEEE Transactions on Learning Technologies'),
        ('https://educationaltechnologyjournal.springeropen.com/articles/most-recent/rss.xml', 'Educational Technology Research'),
        ('https://aied.pub/index.php/IJAIED/gateway/plugin/WebFeedGatewayPlugin/rss2', 'Int. Journal of AI in Education'),
    ]
    
    ai_education_keywords = [
        'artificial intelligence', 'machine learning', 'deep learning',
        'intelligent tutoring', 'adaptive learning', 'personalized',
        'educational data mining', 'learning analytics', 'chatbot',
        'natural language', 'computer vision', 'neural network'
    ]
    
    for rss_url, source_name in sources:
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:4]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:300] if entry.get('summary') else ''
                link = entry.get('link', '')
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_relevant = any(kw in text for kw in ai_education_keywords)
                
                if is_relevant:
                    papers.append({
                        'title': title[:200],
                        'summary': summary,
                        'link': link,
                        'source': source_name,
                        'category': 'AI & EÄŸitim',
                        'needs_translation': True
                    })
        except Exception as e:
            print(f"AI Education RSS hatasÄ± ({source_name}): {e}")
            continue
    
    return papers[:5]

def get_research_papers() -> List[Dict]:
    """
    Akademik araÅŸtÄ±rma makaleleri - SADECE eÄŸitim, matematik, AI ile ilgili
    Ã‡eÅŸitli kaynaklar
    """
    papers = []
    
    # EÄŸitim odaklÄ± kaynaklar
    education_sources = [
        ('https://www.frontiersin.org/journals/education/rss', 'Frontiers in Education'),
        ('https://educationaltechnologyjournal.springeropen.com/articles/most-recent/rss.xml', 'Ed Tech Research'),
        ('https://www.tandfonline.com/feed/rss/cede20', 'Educational Research'),
    ]
    
    # Matematik odaklÄ± kaynaklar
    math_sources = [
        ('https://www.frontiersin.org/journals/applied-mathematics-and-statistics/rss', 'Frontiers Applied Math'),
    ]
    
    # AI odaklÄ± kaynaklar
    ai_sources = [
        ('https://www.nature.com/natmachintell.rss', 'Nature Machine Intelligence'),
        ('http://feeds.nature.com/srep/rss/current', 'Nature Scientific Reports'),
    ]
    
    # EÄŸitim anahtar kelimeleri
    education_keywords = [
        'education', 'learning', 'student', 'teacher', 'school', 'classroom',
        'curriculum', 'pedagogy', 'instruction', 'assessment', 'teaching',
        'academic', 'educational', 'cognitive', 'achievement', 'performance',
        'literacy', 'numeracy', 'stem', 'mathematics education', 'science education',
        'pisa', 'timss', 'evaluation'
    ]
    
    # Matematik anahtar kelimeleri
    math_keywords = [
        'mathematics', 'mathematical', 'algebra', 'geometry', 'calculus',
        'statistics', 'probability', 'theorem', 'proof', 'equation',
        'algorithm', 'computation', 'optimization', 'numerical'
    ]
    
    # AI anahtar kelimeleri
    ai_keywords = [
        'artificial intelligence', 'machine learning', 'deep learning',
        'neural network', 'nlp', 'natural language', 'computer vision',
        'reinforcement learning', 'transformer', 'large language model'
    ]
    
    # Kesinlikle istemediÄŸimiz konular
    exclude_keywords = [
        'cancer', 'tumor', 'disease', 'clinical', 'patient', 'medical',
        'drug', 'therapy', 'cell', 'protein', 'gene', 'virus', 'bacteria',
        'mouse', 'rat', 'animal', 'plant', 'ecology', 'ocean', 'bridge',
        'earthquake', 'geology', 'thyroid', 'seismic', 'fire', 'flood'
    ]
    
    all_sources = education_sources + math_sources + ai_sources
    
    for rss_url, source_name in all_sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:8]:
                title = entry.get('title', '')
                summary = entry.get('summary', '')[:400] if entry.get('summary') else ''
                link = entry.get('link', '')
                published = entry.get('published', '')
                
                if not title:
                    continue
                
                if not is_recent(published, hours=168):  # 1 hafta
                    continue
                
                if deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                
                # En az bir ilgili anahtar kelime iÃ§ermeli
                is_education = any(kw in text for kw in education_keywords)
                is_math = any(kw in text for kw in math_keywords)
                is_ai = any(kw in text for kw in ai_keywords)
                
                # DÄ±ÅŸlanan konular iÃ§ermemeli
                is_excluded = any(kw in text for kw in exclude_keywords)
                
                if (is_education or is_math or is_ai) and not is_excluded:
                    # Kategori belirle
                    if is_education:
                        category = 'EÄŸitim'
                    elif is_math:
                        category = 'Matematik'
                    else:
                        category = 'AI'
                    
                    papers.append({
                        'title': title[:200],
                        'summary': summary[:300],
                        'link': link,
                        'source': source_name,
                        'category': category,
                        'needs_translation': True
                    })
        except Exception as e:
            continue
    
    # Kaynak Ã§eÅŸitliliÄŸi
    final_papers = []
    source_counts = {}
    
    for paper in papers:
        source = paper.get('source', '')
        if source not in source_counts:
            source_counts[source] = 0
        
        if source_counts[source] < 2:
            final_papers.append(paper)
            source_counts[source] += 1
        
        if len(final_papers) >= 8:
            break
    
    return final_papers

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“Š ULUSLARARASI DEÄERLENDÄ°RME RAPORLARI (PISA, TIMSS)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_international_assessment_news() -> List[Dict]:
    """
    PISA, TIMSS ve uluslararasÄ± deÄŸerlendirme haberleri - RSS tabanlÄ±
    """
    news = []
    
    # OECD EÄŸitim RSS
    oecd_sources = [
        ('https://www.oecd.org/education/rss/', 'OECD Education'),
        ('https://oecdedutoday.com/feed/', 'OECD Education Today'),
    ]
    
    pisa_timss_keywords = [
        'pisa', 'timss', 'pirls', 'talis', 'international assessment',
        'student achievement', 'education ranking', 'oecd education',
        'learning outcomes', 'education performance', 'education comparison'
    ]
    
    for rss_url, source in oecd_sources:
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:6]:
                title = entry.get('title', '')
                link = entry.get('link', '')
                summary = entry.get('summary', '')[:200] if entry.get('summary') else ''
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                text = (title + ' ' + summary).lower()
                is_relevant = any(kw in text for kw in pisa_timss_keywords)
                
                if is_relevant:
                    news.append({
                        'title': title[:150],
                        'source': source,
                        'link': link,
                        'type': 'UluslararasÄ± DeÄŸerlendirme',
                        'needs_translation': True
                    })
        except Exception as e:
            print(f"OECD RSS hatasÄ± ({source}): {e}")
            continue
    
    # EÄŸitim karÅŸÄ±laÅŸtÄ±rma haberleri
    comparison_sources = [
        ('https://www.brookings.edu/topic/global-education/feed/', 'Brookings Global Education'),
        ('https://gemreportunesco.wordpress.com/feed/', 'UNESCO GEM Report'),
    ]
    
    for rss_url, source in comparison_sources:
        try:
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:4]:
                title = entry.get('title', '')
                link = entry.get('link', '')
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                news.append({
                    'title': title[:150],
                    'source': source,
                    'link': link,
                    'type': 'Global EÄŸitim',
                    'needs_translation': True
                })
        except:
            continue
    
    return news[:6]

def get_turkey_assessment_research() -> List[Dict]:
    """
    TÃ¼rkiye ulusal izleme ve deÄŸerlendirme araÅŸtÄ±rmalarÄ± - RSS tabanlÄ±
    """
    research = []
    
    # TÃ¼rkiye akademik dergileri RSS
    sources = [
        ('https://dergipark.org.tr/tr/pub/egam/rss', 'EÄŸitimde ve Psikolojide Ã–lÃ§me'),
        ('https://dergipark.org.tr/tr/pub/kefdergi/rss', 'Kastamonu EÄŸitim'),
        ('https://dergipark.org.tr/tr/pub/aod/rss', 'Anadolu Ã–ÄŸretmen'),
        ('https://dergipark.org.tr/tr/pub/ted/rss', 'TÃ¼rk EÄŸitim Bilimleri'),
    ]
    
    keywords = [
        'pisa', 'timss', 'abide', 'lgs', 'yks', 'Ã¶lÃ§me', 'deÄŸerlendirme',
        'baÅŸarÄ±', 'performans', 'matematik', 'fen', 'okuma', 'ulusal'
    ]
    
    for rss_url, source in sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:5]:
                title = entry.get('title', '')
                link = entry.get('link', '')
                
                if not title or deduplicator.is_duplicate(title):
                    continue
                
                text = title.lower()
                is_relevant = any(kw in text for kw in keywords)
                
                if is_relevant:
                    research.append({
                        'title': title[:150],
                        'source': source,
                        'link': link,
                        'type': 'TÃ¼rkiye AraÅŸtÄ±rma'
                    })
        except Exception as e:
            continue
    
    # Sabit Ã¶nemli kaynaklar
    static_sources = [
        {
            'title': 'ABÄ°DE - MEB Akademik Becerilerin Ä°zlenmesi ve DeÄŸerlendirilmesi',
            'source': 'MEB',
            'link': 'https://abide.meb.gov.tr',
            'type': 'Ulusal Ä°zleme'
        },
        {
            'title': 'TEDMEM EÄŸitim DeÄŸerlendirme RaporlarÄ±',
            'source': 'TEDMEM',
            'link': 'https://tedmem.org',
            'type': 'AraÅŸtÄ±rma Merkezi'
        },
        {
            'title': 'ERG EÄŸitim Ä°zleme Raporu',
            'source': 'EÄŸitim Reformu GiriÅŸimi',
            'link': 'https://www.egitimreformugirisimi.org',
            'type': 'Ä°zleme Raporu'
        }
    ]
    
    # Statik kaynaklarÄ± da ekle (yineleme yoksa)
    for item in static_sources:
        if not deduplicator.is_duplicate(item['title']):
            research.append(item)
    
    return research[:6]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“– EÄÄ°TÄ°M DERGÄ° VE KÄ°TAPLARI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_education_journals() -> List[Dict]:
    """
    EÄŸitim dergileri ve yeni kitaplar
    """
    journals = []
    
    # Ã–nemli eÄŸitim dergileri RSS
    sources = [
        ('https://journals.sagepub.com/action/showFeed?ui=0&mi=ehikzz&ai=2b4&jc=rera&type=etoc&feed=rss', 'Review of Educational Research'),
        ('https://www.tandfonline.com/feed/rss/tedp20', 'Educational Psychologist'),
        ('https://www.journals.elsevier.com/computers-and-education/rss', 'Computers & Education'),
        ('https://link.springer.com/search.rss?facet-content-type=Article&facet-journal-id=11423&channel-name=Educational+Technology+Research+and+Development', 'ETR&D'),
    ]
    
    for rss_url, source in sources:
        try:
            feed = feedparser.parse(rss_url)
            for entry in feed.entries[:3]:
                title = entry.get('title', '')
                link = entry.get('link', '')
                
                if title and not deduplicator.is_duplicate(title):
                    journals.append({
                        'title': title[:150],
                        'source': source,
                        'link': link,
                        'type': 'Dergi Makalesi',
                        'needs_translation': True
                    })
        except:
            continue
    
    return journals[:6]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ã–ÄRENCÄ° GÃœNDEMÄ° - DÄ°NAMÄ°K (GerÃ§ek Trend Veriler)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_student_trending_topics() -> List[Dict]:
    """
    Ã–ÄŸrencilerin gerÃ§ekten konuÅŸtuÄŸu konular
    Kaynaklar: EkÅŸi SÃ¶zlÃ¼k, Reddit, Ã–ÄŸrenci ForumlarÄ±, Twitter/X trendleri
    """
    trending = []
    
    # EÄŸitim ile ilgili anahtar kelimeler
    education_keywords = [
        'lgs', 'yks', 'tyt', 'ayt', 'Ã¶sym', 'sÄ±nav', 'okul', 'ders',
        'Ã¶ÄŸretmen', 'Ã¶ÄŸrenci', 'Ã¼niversite', 'lise', 'matematik',
        'fizik', 'kimya', 'biyoloji', 'tÃ¼rkÃ§e', 'tarih', 'coÄŸrafya',
        'mÃ¼fredat', 'meb', 'eÄŸitim', 'kpss', 'ales', 'yds', 'dgs',
        'sÄ±nÄ±f', 'not', 'karne', 'tatil', 'burs', 'yurt', 'kredi',
        'deneme', 'soru', 'konu', 'tercih', 'puan', 'sÄ±ralama',
        'dershane', 'kurs', 'Ã¶dev', 'proje', 'staj', 'mezuniyet'
    ]
    
    # 1. EKÅÄ° SÃ–ZLÃœK - GÃ¼ndem
    print("   ğŸ“± EkÅŸi SÃ¶zlÃ¼k taranÄ±yor...")
    try:
        urls = [
            "https://eksisozluk.com/basliklar/gundem",
            "https://eksisozluk.com/basliklar/debe",  # DÃ¼nÃ¼n en beÄŸenilen entryleri
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        for url in urls:
            try:
                r = requests.get(url, headers=headers, timeout=10)
                if r.status_code == 200:
                    soup = BeautifulSoup(r.text, 'html.parser')
                    
                    # BaÅŸlÄ±k listesini bul
                    topic_links = soup.select('ul.topic-list li a') or soup.select('a.topic-title')
                    
                    for link in topic_links[:30]:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        # Entry sayÄ±sÄ±nÄ± bul
                        small = link.find('small')
                        entry_count = small.get_text(strip=True) if small else ''
                        
                        if title and len(title) > 3 and len(title) < 100:
                            # EÄŸitim ile ilgili mi?
                            if any(kw in title.lower() for kw in education_keywords):
                                if not any(t['topic'].lower() == title.lower() for t in trending):
                                    trending.append({
                                        'topic': title[:80],
                                        'source': 'EkÅŸi SÃ¶zlÃ¼k',
                                        'entry_count': entry_count,
                                        'category': 'GÃ¼ndem',
                                        'link': f"https://eksisozluk.com{href}" if href.startswith('/') else href
                                    })
                time.sleep(0.5)
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ EkÅŸi SÃ¶zlÃ¼k hatasÄ±: {e}")
    
    # 2. REDDIT - r/Turkey, r/KGBTR (Ã¶ÄŸrenci paylaÅŸÄ±mlarÄ±)
    print("   ğŸ“± Reddit taranÄ±yor...")
    try:
        subreddits = [
            'https://www.reddit.com/r/Turkey/hot.json',
            'https://www.reddit.com/r/KGBTR/hot.json',
        ]
        
        headers = {
            'User-Agent': 'EducationBot/3.0 (Educational News Aggregator)'
        }
        
        for subreddit_url in subreddits:
            try:
                r = requests.get(subreddit_url, headers=headers, timeout=10)
                if r.status_code == 200:
                    data = r.json()
                    posts = data.get('data', {}).get('children', [])
                    
                    for post in posts[:25]:
                        post_data = post.get('data', {})
                        title = post_data.get('title', '')
                        score = post_data.get('score', 0)
                        permalink = post_data.get('permalink', '')
                        
                        if title and any(kw in title.lower() for kw in education_keywords):
                            if not any(t['topic'].lower() == title.lower()[:50] for t in trending):
                                trending.append({
                                    'topic': title[:80],
                                    'source': 'Reddit',
                                    'score': f"â¬†ï¸ {score}",
                                    'category': 'Sosyal Medya',
                                    'link': f"https://reddit.com{permalink}"
                                })
                time.sleep(0.5)
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ Reddit hatasÄ±: {e}")
    
    # 3. Ã–ÄRENCÄ° FORUMLARI
    print("   ğŸ“± Ã–ÄŸrenci forumlarÄ± taranÄ±yor...")
    try:
        forums = [
            ('https://www.memurlar.net/haber/egitim/rss/', 'Memurlar.net'),
            ('https://www.kamubiz.com/feed/', 'KamuBiz'),
        ]
        
        for forum_url, forum_name in forums:
            try:
                feed = feedparser.parse(forum_url)
                for entry in feed.entries[:10]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    
                    if title and any(kw in title.lower() for kw in education_keywords):
                        if not any(t['topic'].lower() == title.lower()[:50] for t in trending):
                            trending.append({
                                'topic': title[:80],
                                'source': forum_name,
                                'category': 'Forum',
                                'link': link
                            })
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ Forum hatasÄ±: {e}")
    
    # 4. TWITTER/X TRENDLERÄ° - EÄŸitim hashtagleri
    print("   ğŸ“± Twitter trendleri taranÄ±yor...")
    try:
        # Nitter instance'larÄ± (Twitter alternatifi - API gerektirmez)
        nitter_urls = [
            'https://nitter.poast.org/search?f=tweets&q=%23LGS',
            'https://nitter.poast.org/search?f=tweets&q=%23YKS',
            'https://nitter.poast.org/search?f=tweets&q=%23TYT',
        ]
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        for nitter_url in nitter_urls[:2]:
            try:
                r = requests.get(nitter_url, headers=headers, timeout=8)
                if r.status_code == 200:
                    soup = BeautifulSoup(r.text, 'html.parser')
                    tweets = soup.select('.tweet-content') or soup.select('.timeline-item')
                    
                    for tweet in tweets[:5]:
                        text = tweet.get_text(strip=True)[:100]
                        if text and len(text) > 20:
                            if not any(t['topic'].lower() == text.lower()[:40] for t in trending):
                                trending.append({
                                    'topic': text[:80],
                                    'source': 'Twitter/X',
                                    'category': 'Sosyal Medya'
                                })
                time.sleep(0.5)
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ Twitter hatasÄ±: {e}")
    
    # 5. YOUTUBE - EÄŸitim trendleri
    print("   ğŸ“± YouTube trendleri taranÄ±yor...")
    try:
        # YouTube RSS - PopÃ¼ler eÄŸitim kanallarÄ±
        youtube_channels = [
            ('https://www.youtube.com/feeds/videos.xml?channel_id=UCvMZ2d5r47nGVNPzI6hGX8A', 'TonguÃ§ Akademi'),
            ('https://www.youtube.com/feeds/videos.xml?channel_id=UC6JYy4gZQaoNLbXxBn4cFjg', 'Hocalara Geldik'),
        ]
        
        for channel_url, channel_name in youtube_channels:
            try:
                feed = feedparser.parse(channel_url)
                for entry in feed.entries[:3]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    
                    if title:
                        if not any(t['topic'].lower() == title.lower()[:40] for t in trending):
                            trending.append({
                                'topic': title[:80],
                                'source': f'YouTube - {channel_name}',
                                'category': 'Video',
                                'link': link
                            })
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ YouTube hatasÄ±: {e}")
    
    # 6. GOOGLE TRENDS - TÃ¼rkiye eÄŸitim aramalarÄ±
    print("   ğŸ“± Google Trends kontrol ediliyor...")
    try:
        # Google Trends RSS (varsa)
        trends_url = "https://trends.google.com/trends/trendingsearches/daily/rss?geo=TR"
        feed = feedparser.parse(trends_url)
        
        for entry in feed.entries[:20]:
            title = entry.get('title', '')
            
            if title and any(kw in title.lower() for kw in education_keywords):
                if not any(t['topic'].lower() == title.lower() for t in trending):
                    trending.append({
                        'topic': title[:80],
                        'source': 'Google Trends',
                        'category': 'Arama Trendi'
                    })
    except Exception as e:
        print(f"   âš ï¸ Google Trends hatasÄ±: {e}")
    
    # 7. DONANIM HABER / TEKNOLOJÄ° FORUMLARI (Ã–ÄŸrenci paylaÅŸÄ±mlarÄ±)
    print("   ğŸ“± Teknoloji forumlarÄ± taranÄ±yor...")
    try:
        tech_forums = [
            ('https://forum.donanimhaber.com/rss.ashx?CategoryID=35', 'DonanÄ±m Haber'),
        ]
        
        for forum_url, forum_name in tech_forums:
            try:
                feed = feedparser.parse(forum_url)
                for entry in feed.entries[:10]:
                    title = entry.get('title', '')
                    link = entry.get('link', '')
                    
                    if title and any(kw in title.lower() for kw in education_keywords):
                        if not any(t['topic'].lower() == title.lower()[:40] for t in trending):
                            trending.append({
                                'topic': title[:80],
                                'source': forum_name,
                                'category': 'Forum',
                                'link': link
                            })
            except:
                continue
    except Exception as e:
        print(f"   âš ï¸ Forum hatasÄ±: {e}")
    
    # SonuÃ§larÄ± sÄ±rala - kaynak Ã§eÅŸitliliÄŸine gÃ¶re
    # Her kaynaktan en fazla 2 tane al
    final_trending = []
    source_counts = {}
    
    for item in trending:
        source = item.get('source', 'DiÄŸer')
        if source not in source_counts:
            source_counts[source] = 0
        
        if source_counts[source] < 2:
            final_trending.append(item)
            source_counts[source] += 1
        
        if len(final_trending) >= 10:
            break
    
    # EÄŸer yeterli veri gelmezse fallback
    if len(final_trending) < 3:
        print("   âš ï¸ Yeterli trend bulunamadÄ±, alternatif konular ekleniyor...")
        today = datetime.now()
        
        fallback_topics = [
            {'topic': f'LGS 2026 hazÄ±rlÄ±k stratejileri', 'source': 'Ã–neri', 'category': 'LGS'},
            {'topic': f'YKS tercih dÃ¶neminde dikkat edilecekler', 'source': 'Ã–neri', 'category': 'YKS'},
            {'topic': 'SÄ±nav kaygÄ±sÄ± ile baÅŸa Ã§Ä±kma', 'source': 'Ã–neri', 'category': 'Motivasyon'},
        ]
        
        for topic in fallback_topics:
            if len(final_trending) < 8:
                final_trending.append(topic)
    
    return final_trending[:10]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MOTÄ°VASYON MESAJI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_daily_motivation() -> Dict:
    """GÃ¼nÃ¼n motivasyon mesajÄ±"""
    today = datetime.now()
    day_of_week = today.strftime('%A')
    
    themes = {
        'Monday': 'Hafta baÅŸÄ± enerjisi',
        'Tuesday': 'Hedef belirleme',
        'Wednesday': 'YarÄ± yol motivasyonu',
        'Thursday': 'Azim ve kararlÄ±lÄ±k',
        'Friday': 'Hafta sonu Ã¶ncesi sprint',
        'Saturday': 'Verimli hafta sonu',
        'Sunday': 'Dinlenme ve planlama'
    }
    
    theme = themes.get(day_of_week, 'BaÅŸarÄ±')
    
    if GEMINI_KEY and genai:
        try:
            client = genai.Client(api_key=GEMINI_KEY)
            
            # SÄ±nava kalan gÃ¼n hesapla
            lgs_date = datetime(2026, 6, 14)
            days_left = (lgs_date.date() - today.date()).days
            
            prompt = f"""LGS veya YKS'ye hazÄ±rlanan bir Ã¶ÄŸrenci iÃ§in kÄ±sa ve motive edici bir mesaj yaz.

Tema: {theme}
LGS'ye kalan gÃ¼n: {days_left}

Kurallar:
1. Maksimum 2-3 cÃ¼mle
2. Samimi ve cesaretlendirici
3. Somut Ã§alÄ±ÅŸma Ã¶nerisi iÃ§ersin
4. Emoji kullan
5. TÃ¼rkÃ§e yaz"""

            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )
            
            return {
                'message': response.text.strip(),
                'theme': theme,
                'generated': True
            }
        except Exception as e:
            print(f"Motivasyon hatasÄ±: {e}")
    
    # VarsayÄ±lan
    import random
    messages = [
        "ğŸ’ª Her gÃ¼n bir adÄ±m daha ileri! BugÃ¼n de elinden gelenin en iyisini yap.\nğŸ“š Ã–neri: 25 dakika odaklanarak Ã§alÄ±ÅŸ.",
        "ğŸŒŸ BaÅŸarÄ±, her gÃ¼n yapÄ±lan kÃ¼Ã§Ã¼k adÄ±mlarÄ±n toplamÄ±dÄ±r.\nğŸ“š Ã–neri: ZayÄ±f bir konuyu tekrar et.",
        "ğŸ¯ Hedefe odaklan! Sen baÅŸarabilirsin!\nğŸ“š Ã–neri: BugÃ¼n en az 20 soru Ã§Ã¶z.",
    ]
    
    return {
        'message': random.choice(messages),
        'theme': theme,
        'generated': False
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Ã‡EVÄ°RÄ° FONKSÄ°YONU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def translate_to_turkish(text: str, is_headline: bool = True) -> str:
    """Gemini ile Ã§eviri"""
    if not text or not GEMINI_KEY or not genai:
        return text
    
    try:
        client = genai.Client(api_key=GEMINI_KEY)
        
        prompt = f"""AÅŸaÄŸÄ±daki haber baÅŸlÄ±ÄŸÄ±nÄ± TÃ¼rkÃ§eye Ã§evir.
Teknik terimleri (AI, PISA, STEM, OECD) olduÄŸu gibi bÄ±rak.
Sadece Ã§eviriyi yaz.

Ä°ngilizce: {text}
TÃ¼rkÃ§e:"""
        
        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        
        translated = response.text.strip()
        if translated.lower().startswith("tÃ¼rkÃ§e"):
            translated = translated.split(":", 1)[-1].strip()
        
        return translated if translated else text
    except:
        return text

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GÃœNÃœN Ã–ZETÄ° (AI)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_daily_summary(all_news: Dict) -> str:
    """Gemini ile gÃ¼nlÃ¼k analiz"""
    if not GEMINI_KEY or not genai:
        return ""
    
    try:
        client = genai.Client(api_key=GEMINI_KEY)
        
        news_text = ""
        
        # Haberleri topla
        if all_news.get('turkey_news'):
            news_text += "=== TÃœRKÄ°YE ===\n"
            for n in all_news['turkey_news'][:5]:
                news_text += f"- {n['title']}\n"
        
        if all_news.get('ai_news'):
            news_text += "\n=== AI & EDTECH ===\n"
            for n in all_news['ai_news'][:4]:
                news_text += f"- {n['title']}\n"
        
        if all_news.get('pisa_news'):
            news_text += "\n=== PISA ÃœLKELERÄ° ===\n"
            for country, items in all_news['pisa_news'].items():
                for n in items[:2]:
                    news_text += f"- [{n['country']}] {n['title']}\n"
        
        if all_news.get('papers'):
            news_text += "\n=== ARAÅTIRMALAR ===\n"
            for p in all_news['papers'][:3]:
                news_text += f"- {p['title']}\n"
        
        prompt = f"""Deneyimli bir eÄŸitim analisti olarak aÅŸaÄŸÄ±daki haberleri analiz et:

{news_text}

GÃ–REV: Ã–ÄŸretmen ve Ã¶ÄŸrenciler iÃ§in kÄ±sa bir gÃ¼nlÃ¼k brifing hazÄ±rla:

ğŸ‡¹ğŸ‡· TÃœRKÄ°YE'DE BUGÃœN: (2-3 madde)
ğŸ¤– AI & TEKNOLOJÄ°: (2 madde)
ğŸŒ DÃœNYADAN: (2 madde - PISA Ã¼lkelerinden dersler)
ğŸ’¡ PRATÄ°K Ã–NERÄ°: (1 madde)

Kurallar:
- Her madde 1 cÃ¼mle
- Haberleri yorumla, sadece Ã¶zetleme
- TÃ¼rkÃ§e, akÄ±cÄ± dil
- Toplam 200 kelime"""

        response = client.models.generate_content(
            model="gemini-2.0-flash",
            contents=prompt
        )
        
        return response.text.strip()
    except Exception as e:
        print(f"Ã–zet hatasÄ±: {e}")
        return ""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RAPOR OLUÅTURMA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def generate_report() -> str:
    """GÃ¼nlÃ¼k eÄŸitim raporu"""
    
    # Her raporda deduplicator'Ä± sÄ±fÄ±rla
    deduplicator.reset()
    
    report = []
    today = datetime.now()
    
    # BaÅŸlÄ±k
    report.append("â•" * 50)
    report.append("ğŸ“š EÄÄ°TÄ°M GÃœNDEM RAPORU v3.0")
    report.append(f"ğŸ“… {format_turkish_date(today, include_day=True)}")
    report.append("â•" * 50)
    report.append("")
    
    # 1. SINAV TAKVÄ°MÄ°
    print("ğŸ“… SÄ±nav takvimi...")
    countdown = get_exam_countdown()
    
    report.append("â”" * 50)
    report.append("â° SINAV TAKVÄ°MÄ° & GERÄ° SAYIM")
    report.append("â”" * 50)
    
    for item in countdown['countdowns']:
        if item['is_exam']:
            report.append(f"\n{item['name']}")
            report.append(f"   ğŸ“† {item['date']}")
            report.append(f"   â³ {item['time_str']}")
    
    report.append("")
    
    # 2. TÃœRKÄ°YE EÄÄ°TÄ°M GÃœNDEMÄ°
    print("ğŸ‡¹ğŸ‡· TÃ¼rkiye haberleri...")
    meb_news = get_meb_news()
    turkey_news = get_education_news_turkey()
    
    report.append("â”" * 50)
    report.append("ğŸ›ï¸ MEB & TÃœRKÄ°YE EÄÄ°TÄ°M GÃœNDEMÄ°")
    report.append("â”" * 50)
    
    all_turkey = meb_news + turkey_news
    shown = 0
    for news in all_turkey[:8]:
        if shown >= 6:
            break
        icon = "ğŸ”´" if news.get('is_exam_related') else "ğŸ“°"
        report.append(f"\n{icon} {news['title']}")
        report.append(f"   ğŸ“ {news['source']}")
        if news.get('link'):
            report.append(f"   ğŸ”— {news.get('link', '')}")
        shown += 1
    
    report.append("")
    
    # 3. YAPAY ZEKA & EÄÄ°TÄ°M TEKNOLOJÄ°SÄ°
    print("ğŸ¤– AI haberleri...")
    ai_news = get_ai_education_news()
    
    report.append("â”" * 50)
    report.append("ğŸ¤– YAPAY ZEKA & EÄÄ°TÄ°M TEKNOLOJÄ°SÄ°")
    report.append("â”" * 50)
    
    # LLM ve AI geliÅŸmelerini ayÄ±r
    llm_news = [n for n in ai_news if n.get('is_llm') or n.get('category') in ['LLM', 'AI Haber', 'AraÅŸtÄ±rma']]
    edtech_news = [n for n in ai_news if n.get('category') in ['EdTech', 'YÃ¼ksekÃ¶ÄŸretim']]
    
    translate_count = 0
    
    # LLM GeliÅŸmeleri
    if llm_news:
        report.append("\nğŸ§  BÃœYÃœK DÄ°L MODELLERÄ° & AI GELÄ°ÅMELERÄ°:")
        for news in llm_news[:4]:
            if news.get('needs_translation') and translate_count < 4:
                title_tr = translate_to_turkish(news['title'])
                translate_count += 1
                time.sleep(0.3)
            else:
                title_tr = news['title']
            
            category_icon = {
                'LLM': 'ğŸ”®',
                'AI Haber': 'ğŸ“°',
                'AraÅŸtÄ±rma': 'ğŸ”¬'
            }.get(news.get('category', ''), 'ğŸ”¹')
            
            report.append(f"\n{category_icon} {title_tr[:90]}")
            report.append(f"   ğŸ“ {news['source']} ({news.get('category', '')})")
            if news.get('link'):
                report.append(f"   ğŸ”— {news.get('link', '')}")
    
    # EdTech Haberleri
    if edtech_news:
        report.append("\nğŸ“± EÄÄ°TÄ°M TEKNOLOJÄ°SÄ° (EdTech):")
        for news in edtech_news[:3]:
            if news.get('needs_translation') and translate_count < 6:
                title_tr = translate_to_turkish(news['title'])
                translate_count += 1
                time.sleep(0.3)
            else:
                title_tr = news['title']
            
            report.append(f"\nğŸ”¹ {title_tr[:90]}")
            report.append(f"   ğŸ“ {news['source']}")
            if news.get('link'):
                report.append(f"   ğŸ”— {news.get('link', '')}")
    
    # Gemini ile AI geliÅŸmelerinin eÄŸitimde kullanÄ±m analizi
    if ai_news and GEMINI_KEY and genai:
        print("   ğŸ¤– AI geliÅŸmeleri eÄŸitim analizi yapÄ±lÄ±yor...")
        try:
            client = genai.Client(api_key=GEMINI_KEY)
            
            # Haberleri topla
            news_titles = [n['title'] for n in ai_news[:6]]
            news_text = "\n".join([f"- {t}" for t in news_titles])
            
            prompt = f"""AÅŸaÄŸÄ±daki yapay zeka ve eÄŸitim teknolojisi haberlerini analiz et:

{news_text}

GÃ–REV: Bu geliÅŸmelerin TÃ¼rkiye'deki Ã¶ÄŸretmen ve Ã¶ÄŸrenciler iÃ§in pratik uygulamalarÄ±nÄ± 3-4 maddede Ã¶zetle.

Format:
ğŸ’¡ [KÄ±sa baÅŸlÄ±k]: [1 cÃ¼mle aÃ§Ä±klama]

Kurallar:
- Her madde 1-2 cÃ¼mle
- Pratik ve uygulanabilir Ã¶neriler
- TÃ¼rkÃ§e yaz
- Toplam 100 kelime"""

            response = client.models.generate_content(
                model="gemini-2.0-flash",
                contents=prompt
            )
            
            if response.text:
                report.append("\nğŸ“Š EÄÄ°TÄ°MDE KULLANIM ANALÄ°ZÄ° (Gemini):")
                report.append(response.text.strip())
        except Exception as e:
            print(f"   âš ï¸ AI analiz hatasÄ±: {e}")
    
    report.append("")
    
    # 4. MATEMATÄ°K DÃœNYASINDAN
    print("â• Matematik haberleri...")
    math_news = get_math_news()
    
    report.append("â”" * 50)
    report.append("â• MATEMATÄ°K DÃœNYASINDAN")
    report.append("â”" * 50)
    
    for news in math_news[:4]:
        if news.get('needs_translation') and translate_count < 5:
            title_tr = translate_to_turkish(news['title'])
            translate_count += 1
            time.sleep(0.3)
        else:
            title_tr = news['title']
        
        report.append(f"\nğŸ“ {title_tr[:90]}")
        report.append(f"   ğŸ“ {news['source']}")
        if news.get('link'):
            report.append(f"   ğŸ”— {news['link']}")
    
    report.append("")
    
    # 4.5. ğŸ¬ YOUTUBE AI VÄ°DEOLARI - YENÄ° BÃ–LÃœM
    print("ğŸ¬ YouTube AI videolarÄ± Ã§ekiliyor...")
    youtube_videos = get_youtube_ai_videos()
    
    report.append("â”" * 50)
    report.append("ğŸ¬ YOUTUBE'DA AI KANALLARI")
    report.append("â”" * 50)
    report.append("ğŸ“º Takip edilmesi Ã¶nerilen popÃ¼ler AI kanallarÄ±:")
    
    if youtube_videos:
        # Kaynak tÃ¼rÃ¼ne gÃ¶re kontrol
        is_curated = any(v.get('source') == 'curated' for v in youtube_videos)
        
        if is_curated:
            # Curated liste - kategorilere gÃ¶re grupla
            report.append("\nğŸ”¬ ARAÅTIRMA & TEKNÄ°K:")
            research = [v for v in youtube_videos if v.get('category') in ['AI AraÅŸtÄ±rma', 'Matematik/AI']]
            for video in research[:4]:
                report.append(f"\nâ–¶ï¸ {video['channel']} ({video['subscribers']})")
                report.append(f"   ğŸ“ {video.get('title', '').replace('ğŸ”¥ ', '')}")
                report.append(f"   ğŸ”— {video['link']}")
            
            report.append("\nğŸ› ï¸ AI ARAÃ‡LAR & PRATÄ°K:")
            tools = [v for v in youtube_videos if v.get('category') in ['AI AraÃ§lar', 'AI Prompt']]
            for video in tools[:3]:
                report.append(f"\nâ–¶ï¸ {video['channel']} ({video['subscribers']})")
                report.append(f"   ğŸ“ {video.get('title', '').replace('ğŸ”¥ ', '')}")
                report.append(f"   ğŸ”— {video['link']}")
            
            report.append("\nğŸ“° HABER & ANALÄ°Z:")
            news = [v for v in youtube_videos if v.get('category') in ['AI Analiz', 'Tech/AI', 'AI Podcast']]
            for video in news[:3]:
                report.append(f"\nâ–¶ï¸ {video['channel']} ({video['subscribers']})")
                report.append(f"   ğŸ“ {video.get('title', '').replace('ğŸ”¥ ', '')}")
                report.append(f"   ğŸ”— {video['link']}")
            
            report.append("\nğŸ¢ RESMÄ° KANALLAR:")
            official = [v for v in youtube_videos if v.get('category') == 'Resmi']
            for video in official[:2]:
                report.append(f"\nâ–¶ï¸ {video['channel']} ({video['subscribers']})")
                report.append(f"   ğŸ“ {video.get('title', '').replace('ğŸ”¥ ', '')}")
                report.append(f"   ğŸ”— {video['link']}")
            
            report.append(f"\nğŸ’¡ Bu kanallarÄ± YouTube'da takip ederek AI dÃ¼nyasÄ±ndaki")
            report.append(f"   son geliÅŸmelerden haberdar olabilirsiniz!")
        else:
            # RSS'den Ã§ekilen gerÃ§ek videolar
            report.append("\nğŸ“¹ SON YAYINLANAN AI VÄ°DEOLARI:")
            
            research_videos = [v for v in youtube_videos if v.get('category') in ['AI AraÅŸtÄ±rma', 'Matematik/AI']]
            tools_videos = [v for v in youtube_videos if v.get('category') in ['AI AraÃ§lar', 'AI Prompt']]
            news_videos = [v for v in youtube_videos if v.get('category') in ['AI Analiz', 'Tech/AI', 'AI Podcast']]
            official_videos = [v for v in youtube_videos if v.get('category') == 'Resmi']
            
            if research_videos:
                report.append("\nğŸ”¬ ARAÅTIRMA & TEKNÄ°K:")
                for video in research_videos[:3]:
                    report.append(f"\nâ–¶ï¸ {video['title']}")
                    report.append(f"   ğŸ“º {video['channel']} ({video['subscribers']})")
                    report.append(f"   ğŸ”— {video['link']}")
            
            if tools_videos:
                report.append("\nğŸ› ï¸ AI ARAÃ‡LAR & PRATÄ°K:")
                for video in tools_videos[:3]:
                    report.append(f"\nâ–¶ï¸ {video['title']}")
                    report.append(f"   ğŸ“º {video['channel']} ({video['subscribers']})")
                    report.append(f"   ğŸ”— {video['link']}")
            
            if news_videos:
                report.append("\nğŸ“° HABER & ANALÄ°Z:")
                for video in news_videos[:3]:
                    report.append(f"\nâ–¶ï¸ {video['title']}")
                    report.append(f"   ğŸ“º {video['channel']} ({video['subscribers']})")
                    report.append(f"   ğŸ”— {video['link']}")
            
            if official_videos:
                report.append("\nğŸ¢ RESMÄ° DUYURULAR:")
                for video in official_videos[:2]:
                    report.append(f"\nâ–¶ï¸ {video['title']}")
                    report.append(f"   ğŸ“º {video['channel']}")
                    report.append(f"   ğŸ”— {video['link']}")
            
            report.append(f"\nğŸ“Š Toplam {len(youtube_videos)} yeni AI videosu bulundu")
    else:
        report.append("\nâ€¢ Åu an yeni AI videosu bulunamadÄ±")
    
    report.append("")
    
    # 5. PISA LÄ°DERLERÄ°NDEN
    print("ğŸ† PISA liderleri haberleri...")
    pisa_news = get_pisa_leaders_news()
    
    report.append("â”" * 50)
    report.append("ğŸ† PISA LÄ°DERLERÄ°NDEN EÄÄ°TÄ°M HABERLERÄ°")
    report.append("â”" * 50)
    
    for country_code, news_list in pisa_news.items():
        for news in news_list[:2]:
            if translate_count < 8:
                title_tr = translate_to_turkish(news['title'])
                translate_count += 1
                time.sleep(0.3)
            else:
                title_tr = news['title']
            
            report.append(f"\n{news['flag']} {news['country']} ({news['rank']})")
            report.append(f"   {title_tr[:85]}")
            report.append(f"   ğŸ“ {news['source']}")
            if news.get('link'):
                report.append(f"   ğŸ”— {news.get('link', '')}")
    
    report.append("")
    
    # 6. GLOBAL MAKRO HABERLER
    print("ğŸŒ Global haberler...")
    global_news = get_global_macro_education_news()
    
    report.append("â”" * 50)
    report.append("ğŸŒ DÃœNYADAN EÄÄ°TÄ°M POLÄ°TÄ°KALARI")
    report.append("â”" * 50)
    
    for news in global_news[:4]:
        if translate_count < 10:
            title_tr = translate_to_turkish(news['title'])
            translate_count += 1
            time.sleep(0.3)
        else:
            title_tr = news['title']
        
        report.append(f"\nğŸ”¸ {title_tr[:90]}")
        report.append(f"   ğŸ“ {news['source']} ({news.get('category', '')})")
    
    report.append("")
    
    # 7. BÄ°LÄ°MSEL MAKALELER
    print("ğŸ“„ Bilimsel makaleler...")
    arxiv_papers = get_arxiv_papers_safe()
    eric_papers = get_eric_papers()
    research_papers = get_research_papers()
    
    report.append("â”" * 50)
    report.append("ğŸ“„ BÄ°LÄ°MSEL MAKALELER & ARAÅTIRMALAR")
    report.append("â”" * 50)
    
    # arXiv
    if arxiv_papers:
        report.append("\nğŸ“ arXiv - EÄÄ°TÄ°M & AI:")
        for paper in arxiv_papers[:3]:
            if translate_count < 12:
                title_tr = translate_to_turkish(paper['title'])
                translate_count += 1
                time.sleep(0.3)
            else:
                title_tr = paper['title']
            report.append(f"\nğŸ“‘ {title_tr[:100]}")
            if paper.get('link'):
                report.append(f"   ğŸ”— {paper['link']}")
    
    # ERIC
    if eric_papers:
        report.append("\nğŸ“š EÄÄ°TÄ°M ARAÅTIRMALARI:")
        for paper in eric_papers[:2]:
            if translate_count < 14:
                title_tr = translate_to_turkish(paper['title'])
                translate_count += 1
                time.sleep(0.3)
            else:
                title_tr = paper['title']
            report.append(f"\nğŸ“– {title_tr[:100]}")
            if paper.get('link'):
                report.append(f"   ğŸ”— {paper['link']}")
    
    # Research papers - kategoriye gÃ¶re grupla
    if research_papers:
        # Kategorilere ayÄ±r
        edu_papers = [p for p in research_papers if p.get('category') == 'EÄŸitim']
        math_papers = [p for p in research_papers if p.get('category') == 'Matematik']
        ai_papers = [p for p in research_papers if p.get('category') == 'AI']
        
        if edu_papers:
            report.append("\nğŸ“ EÄÄ°TÄ°M BÄ°LÄ°MLERÄ°:")
            for paper in edu_papers[:2]:
                if translate_count < 16:
                    title_tr = translate_to_turkish(paper['title'])
                    translate_count += 1
                    time.sleep(0.3)
                else:
                    title_tr = paper['title']
                report.append(f"\nğŸ“– {title_tr[:100]}")
                report.append(f"   ğŸ“ {paper['source']}")
        
        if math_papers:
            report.append("\nğŸ“ MATEMATÄ°K ARAÅTIRMALARI:")
            for paper in math_papers[:2]:
                if translate_count < 18:
                    title_tr = translate_to_turkish(paper['title'])
                    translate_count += 1
                    time.sleep(0.3)
                else:
                    title_tr = paper['title']
                report.append(f"\nğŸ“– {title_tr[:100]}")
                report.append(f"   ğŸ“ {paper['source']}")
        
        if ai_papers:
            report.append("\nğŸ¤– YAPAY ZEKA ARAÅTIRMALARI:")
            for paper in ai_papers[:2]:
                if translate_count < 20:
                    title_tr = translate_to_turkish(paper['title'])
                    translate_count += 1
                    time.sleep(0.3)
                else:
                    title_tr = paper['title']
                report.append(f"\nğŸ“– {title_tr[:100]}")
                report.append(f"   ğŸ“ {paper['source']}")
    
    report.append("")
    
    # 8. ULUSLARARASI DEÄERLENDÄ°RME
    print("ğŸ“Š UluslararasÄ± deÄŸerlendirme...")
    assessment_news = get_international_assessment_news()
    turkey_research = get_turkey_assessment_research()
    
    report.append("â”" * 50)
    report.append("ğŸ“Š ULUSLARARASI DEÄERLENDÄ°RME (PISA/TIMSS)")
    report.append("â”" * 50)
    
    if assessment_news:
        for item in assessment_news[:3]:
            report.append(f"\nğŸ“ˆ {item['title'][:90]}")
            report.append(f"   ğŸ“ {item['source']} ({item.get('type', '')})")
    
    if turkey_research:
        report.append("\nğŸ‡¹ğŸ‡· TÃœRKÄ°YE ULUSAL Ä°ZLEME:")
        for item in turkey_research[:2]:
            report.append(f"\nğŸ“‹ {item['title'][:90]}")
            report.append(f"   ğŸ“ {item['source']}")
    
    report.append("")
    
    # 9. Ã–ÄRENCÄ° GÃœNDEMÄ°
    print("ğŸ”¥ Ã–ÄŸrenci gÃ¼ndemi (sosyal medya, forumlar)...")
    trending = get_student_trending_topics()
    
    report.append("â”" * 50)
    report.append("ğŸ”¥ Ã–ÄRENCÄ° GÃœNDEMÄ° (Trend Konular)")
    report.append("â”" * 50)
    
    if trending:
        for topic in trending[:8]:
            source = topic.get('source', '')
            category = topic.get('category', '')
            score = topic.get('score', '')
            entry_count = topic.get('entry_count', '')
            link = topic.get('link', '')
            
            # Kaynak ikonu
            source_icon = {
                'EkÅŸi SÃ¶zlÃ¼k': 'ğŸ“—',
                'Reddit': 'ğŸ”´',
                'Twitter/X': 'ğŸ¦',
                'YouTube': 'â–¶ï¸',
                'Google Trends': 'ğŸ“ˆ',
                'Forum': 'ğŸ’¬',
            }.get(topic.get('source', '').split(' - ')[0], 'ğŸ“Œ')
            
            line = f"\n{source_icon} {topic['topic']}"
            
            # Meta bilgiler
            meta = []
            if source:
                meta.append(source)
            if entry_count:
                meta.append(f"{entry_count} entry")
            if score:
                meta.append(score)
            if category:
                meta.append(f"[{category}]")
            
            if meta:
                report.append(line)
                report.append(f"   ğŸ“ {' | '.join(meta)}")
                if link:
                    report.append(f"   ğŸ”— {link}")
            else:
                report.append(line)
    else:
        report.append("\nâ€¢ Åu an aktif trend konusu bulunamadÄ±")
    
    report.append("")
    
    # 10. MOTÄ°VASYON
    print("ğŸ’ª Motivasyon...")
    motivation = get_daily_motivation()
    
    report.append("â”" * 50)
    report.append("ğŸ’ª GÃœNÃœN MOTÄ°VASYONU")
    report.append("â”" * 50)
    report.append("")
    report.append(motivation['message'])
    report.append("")
    
    # 11. GÃœNÃœN Ã–ZETÄ°
    print("ğŸ“ GÃ¼nÃ¼n Ã¶zeti...")
    all_news_data = {
        'turkey_news': meb_news + turkey_news,
        'ai_news': ai_news,
        'pisa_news': pisa_news,
        'papers': arxiv_papers + eric_papers + research_papers
    }
    summary = generate_daily_summary(all_news_data)
    
    if summary:
        report.append("â”" * 50)
        report.append("ğŸ“Š GÃœNÃœN ANALÄ°ZÄ°")
        report.append("â”" * 50)
        report.append("")
        report.append(summary)
        report.append("")
    
    # Son
    report.append("â•" * 50)
    report.append("ğŸ“š Ä°yi Ã§alÄ±ÅŸmalar! BaÅŸarÄ±lar dileriz. ğŸŒŸ")
    report.append("â•" * 50)
    report.append(f"â° Rapor: {datetime.now().strftime('%H:%M:%S')}")
    
    return '\n'.join(report)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TELEGRAM GÃ–NDERÄ°M
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def send_telegram_message(message: str) -> bool:
    """Telegram'a mesaj gÃ¶nder - HTML taglarÄ± temizlenmiÅŸ"""
    if not TELEGRAM_TOKEN or not TELEGRAM_CHAT_ID:
        print("âš ï¸ Telegram ayarlarÄ± eksik!")
        return False
    
    try:
        # HTML taglarÄ±nÄ± temizle (Telegram sadece belirli taglarÄ± destekler)
        # Desteklenen: <b>, <i>, <u>, <s>, <code>, <pre>, <a>
        # Desteklenmeyen taglarÄ± kaldÄ±r
        import re
        
        def clean_html(text):
            # Desteklenmeyen HTML taglarÄ±nÄ± kaldÄ±r
            unsupported_tags = [
                'cite', 'span', 'div', 'p', 'br', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
                'ul', 'ol', 'li', 'table', 'tr', 'td', 'th', 'img', 'figure', 'figcaption',
                'blockquote', 'em', 'strong', 'small', 'sub', 'sup', 'mark', 'del', 'ins',
                'article', 'section', 'header', 'footer', 'nav', 'aside', 'main'
            ]
            
            for tag in unsupported_tags:
                # AÃ§Ä±lÄ±ÅŸ ve kapanÄ±ÅŸ taglarÄ±nÄ± kaldÄ±r
                text = re.sub(f'<{tag}[^>]*>', '', text, flags=re.IGNORECASE)
                text = re.sub(f'</{tag}>', '', text, flags=re.IGNORECASE)
            
            # Kalan HTML entity'leri dÃ¼zelt
            text = text.replace('&nbsp;', ' ')
            text = text.replace('&amp;', '&')
            text = text.replace('&lt;', '<')
            text = text.replace('&gt;', '>')
            text = text.replace('&quot;', '"')
            
            # Birden fazla boÅŸluÄŸu tek boÅŸluÄŸa indir
            text = re.sub(r' +', ' ', text)
            
            return text
        
        # MesajÄ± temizle
        message = clean_html(message)
        
        max_length = 4000
        parts = []
        
        if len(message) <= max_length:
            parts = [message]
        else:
            lines = message.split('\n')
            current_part = ""
            
            for line in lines:
                if len(current_part) + len(line) + 1 <= max_length:
                    current_part += line + '\n'
                else:
                    if current_part:
                        parts.append(current_part.strip())
                    current_part = line + '\n'
            
            if current_part:
                parts.append(current_part.strip())
        
        url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        
        for i, part in enumerate(parts):
            # HTML parse modunu kapat - dÃ¼z metin gÃ¶nder
            payload = {
                'chat_id': TELEGRAM_CHAT_ID,
                'text': part,
                'disable_web_page_preview': True
                # parse_mode kaldÄ±rÄ±ldÄ± - dÃ¼z metin olarak gÃ¶nder
            }
            
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code != 200:
                print(f"âŒ Telegram hatasÄ± (parÃ§a {i+1}): {response.text}")
                # HTML ile tekrar dene
                payload['parse_mode'] = 'HTML'
                payload['text'] = clean_html(part)
                response = requests.post(url, json=payload, timeout=30)
                
                if response.status_code != 200:
                    print(f"âŒ Telegram HTML hatasÄ±: {response.text}")
                    return False
            
            if i < len(parts) - 1:
                time.sleep(1)
        
        print(f"âœ… Telegram'a {len(parts)} parÃ§a gÃ¶nderildi")
        return True
        
    except Exception as e:
        print(f"âŒ Telegram hatasÄ±: {e}")
        return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ANA PROGRAM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Ana program"""
    print("=" * 50)
    print("ğŸ“š EÄÄ°TÄ°M GÃœNDEM TAKÄ°P BOTU v4.0 - YouTube AI Edition")
    print("=" * 50)
    print("")
    
    report = generate_report()
    print("\n" + report)
    
    if TELEGRAM_TOKEN and TELEGRAM_CHAT_ID:
        print("\nğŸ“¤ Telegram'a gÃ¶nderiliyor...")
        send_telegram_message(report)
    else:
        print("\nâš ï¸ Telegram ayarlarÄ± yapÄ±lmamÄ±ÅŸ.")
    
    print("\nâœ… Bot tamamlandÄ±!")

if __name__ == "__main__":
    main()
